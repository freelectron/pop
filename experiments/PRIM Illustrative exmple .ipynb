{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning Robust with Learnt Models  (PRIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Illustration of why once would want to incorporate robustness when using a learnt/imperfect environment model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import gym\n",
    "from gym.envs.toy_text import discrete\n",
    "# Compute Wilson CI\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# NUmerical optimization \n",
    "from scipy.optimize import minimize, Bounds, NonlinearConstraint, LinearConstraint\n",
    "# Wasserstein distance here accepts samples from mu and v distributions (aka observations)\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import cdist\n",
    "# Use optimal transport library if you wanna give pmf's\n",
    "import ot\n",
    "from ot.datasets import make_1D_gauss as gauss\n",
    "# Fitting lines of the histrograms \n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "os.chdir(\"../\")\n",
    "from agents.agent_qlearning import AgentQLearning, plot_gridworld_value\n",
    "from agents.agent_dp import AgentDP\n",
    "\n",
    "# Extra\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(4)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "########################################\n",
    "# Plotting: workshop paper (two columns)\n",
    "# Some nice guidlines at https://www.dmcdougall.co.uk/publication-ready-the-first-time-beautiful-reproducible-plots-with-matplotlib\n",
    "########################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import text\n",
    "from matplotlib import rcParams\n",
    "# import pylab as plot\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\") #('fast') #('fivethirtyeight') #('bmh') \n",
    "\n",
    "rcParams['axes.labelsize'] = 22\n",
    "rcParams['xtick.labelsize'] = 20\n",
    "rcParams['ytick.labelsize'] = 20\n",
    "rcParams['legend.fontsize'] = 25\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.serif'] = ['Computer Modern Roman']\n",
    "rcParams['text.usetex'] = False # True\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10203040\n",
    "NU_LEFT = 0.45\n",
    "REWARD_LEFT_GOOD = 2\n",
    "REWARD_LEFT_BAD = 0\n",
    "REWARD = 1\n",
    "EXPERIENCE_VOLUMES = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 4000, 6000] \n",
    "ALPHA_CI = 0.1\n",
    "EPSILON_ROBUST = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL Variable for an agent \n",
    "AGENT = AgentQLearning #AgentDPCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to decrease alpha when training Q-learning\n",
    "ALPHA_DEGRAD_DICT = {201:0.1, 501: 0.05, 10001: 0.01}\n",
    "ALPHA_LIST = [0] + list(ALPHA_DEGRAD_DICT.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "class StateGame(discrete.DiscreteEnv):\n",
    "\n",
    "    def __init__(self, nu_left=NU_LEFT, reward_left_good=REWARD_LEFT_GOOD, reward_left_bad=REWARD_LEFT_BAD, reward=REWARD, nu_right=1, seed=None):\n",
    "        \"\"\"\n",
    "        TODO: give terminal states as a parameter to the constructor. \n",
    "        \"\"\"\n",
    "        nS = 4\n",
    "        nA = 2\n",
    "        P = {0: {0: [(nu_left, 1, reward_left_good, True), (1-nu_left, 2, reward_left_bad, True)],\n",
    "                 1: [(nu_right, 3, reward, True)]},\n",
    "             1: {0: [(1, 1, 0, True)],\n",
    "                 1: [(1, 1, 0, True)]},\n",
    "             2: {0: [(1, 2, 0, True)],\n",
    "                 1: [(1, 2, 0, True)]},\n",
    "             3: {0: [(1, 3, 0, True)],\n",
    "                 1: [(1, 3, 0, True)]},\n",
    "             }\n",
    "        \n",
    "        # Initial state distribution: always start at state 0 \n",
    "        isd = np.zeros(nS)\n",
    "        isd[0] = 1.0\n",
    "        \n",
    "        super(StateGame, self).__init__(nS, nA, P, isd)   \n",
    "        \n",
    "        # nT is NSMDP parameter for evolution of transition dynamics within an epoch\n",
    "        # We say that our dynamics stay constant within an episode/epoch\n",
    "        self.nT = 1 \n",
    "        # Transition matrix of the shape S x A x nT x S\n",
    "        self.T = None\n",
    "        # What are the terminal states\n",
    "        self.terminal_states = [1, 2, 3]\n",
    "        self.nu_left = nu_left\n",
    "        self.nu_right = nu_right \n",
    "        self.reward_left_good = reward_left_good\n",
    "        self.reward_left_bad = reward_left_bad\n",
    "        self.reward = reward\n",
    "        self.seed = seed\n",
    "        \n",
    "    def reachable_states(self, s, a):\n",
    "        \"\"\"\n",
    "        :param s: state index\n",
    "        :param a: action index\n",
    "        :return: list of shape nS where 1 is the state index where we can transition to\n",
    "        \"\"\"\n",
    "        transitions = self.P[s][a]\n",
    "        rs = np.zeros(shape=self.nS, dtype=int)\n",
    "        for transition in transitions:\n",
    "            rs[transition[1]] = 1\n",
    "\n",
    "        return rs\n",
    "    \n",
    "    def generate_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        Generate matrix of shape S x A x nT x S\n",
    "        \"\"\"\n",
    "        T = np.zeros(shape=(self.nS, self.nA, self.nT, self.nS), dtype=float)\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                T[s, a, 0, :] = np.zeros(shape=self.nS)\n",
    "                # We are at a state and with action a we can transition to some states\n",
    "                rs = self.reachable_states(s, a)\n",
    "                nrs = sum(rs)\n",
    "                if nrs == 1:\n",
    "                    # We can only transition to a single (possible) state for this action\n",
    "                    # Transition tuple (transition_probability, next_state, reward, done_flag)\n",
    "                    s_p = self.P[s][a][0][1]\n",
    "                    T[s, a, 0, s_p] = 1.0\n",
    "                    \n",
    "                else:\n",
    "                    # More states reachable, redistribute that 1 prob. mass to all the possible ones\n",
    "                    w0 = np.array(T[s, a, 0, :])\n",
    "                    for transition in self.P[s][a]:\n",
    "                        # Assign next state the prob of that state\n",
    "                        w0[transition[1]] = transition[0]\n",
    "                    T[s, a, 0, :] = w0\n",
    "        # Save it\n",
    "        self.T = T\n",
    "\n",
    "        return T\n",
    "    \n",
    "    def perform_reset(self, start_state):\n",
    "        \"\"\" \n",
    "        Wrapper around reset() to comply with previously chosen format. \n",
    "        \n",
    "        Args: \n",
    "            start_state: because we assume in toy problem that we always start from 0 state, we never need \n",
    "                         to change the start state.\n",
    "        \"\"\"\n",
    "        return self.reset(), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 1, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if works\n",
    "env = StateGame()\n",
    "env.reset()\n",
    "env.step(0)\n",
    "env.generate_transition_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate experience\n",
    "env = StateGame()\n",
    "env.reset()\n",
    "\n",
    "experience = []\n",
    "# Set the number of episodes\n",
    "num_experience_tuples = 100\n",
    "\n",
    "# Generate an episode\n",
    "for t in range(num_experience_tuples): \n",
    "    state = env.reset()\n",
    "    # Random action \n",
    "    action = np.random.randint(2)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    # Record experience\n",
    "    experience.append((state, action, reward, next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDict(defaultdict):\n",
    "    \"\"\"\n",
    "    Needed to create disctionaries for the model being learnt.\n",
    "    \"\"\"\n",
    "    def __call__(self):\n",
    "        return DeepDict(self.default_factory)\n",
    "\n",
    "class ModelStateGame(discrete.DiscreteEnv):\n",
    "    \n",
    "    def __init__(self, experience_tuples, reward_left_good=REWARD_LEFT_GOOD, reward_left_bad=REWARD_LEFT_BAD, reward=REWARD, nu_right=1, \n",
    "                 terminal_states=[1, 2, 3], seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            experience_tuples (list): list of tuples of the format (state, a, reward, state_prime)\n",
    "        \"\"\"\n",
    "        self.experience_arr = np.array(experience_tuples)\n",
    "        self.epxerience_dict = None\n",
    "        nS = 4\n",
    "        nA = 2\n",
    "        # Empty transition dictionary\n",
    "        P = None\n",
    "        # Number of time steps, in NSMDP paper, we consider it as time steps when possible disturbances within episode\n",
    "        self.nT = 1\n",
    "        # Same initial state distribution as in the true environment \n",
    "        isd = np.zeros(nS)\n",
    "        isd[0] = 1.0\n",
    "        # Transition matrix \n",
    "        self.T = None\n",
    "\n",
    "        super(ModelStateGame, self).__init__(nS, nA, P, isd)\n",
    "        self.experience_dict = None\n",
    "        self.reward_left_good = reward_left_good\n",
    "        self.reward_left_bad = reward_left_bad\n",
    "        self.reward = reward\n",
    "        self.terminal_states = terminal_states\n",
    "        self.seed(seed)\n",
    "    \n",
    "    def _create_experience_dict(self):\n",
    "        \"\"\"\n",
    "        'Utility' function that procesess list with experience tupples and creates a ditionary that stores\n",
    "        counts of s-a pairs and counts of s.\n",
    "        \"\"\"\n",
    "        experience_arr = self.experience_arr.copy()\n",
    "        self.experience_dict = DeepDict(DeepDict(DeepDict(list)))\n",
    "\n",
    "        # For each possible state and action\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                s_a_experience = experience_arr[\n",
    "                                 np.where((experience_arr[:, 0] == s) * (experience_arr[:, 1] == a))]\n",
    "                self.experience_dict[s][a][\"count\"] = np.zeros(self.nS)\n",
    "                # There is experience for this s, a\n",
    "                if s_a_experience.shape[0] > 0:\n",
    "                    unique, counts = np.unique(s_a_experience[:, 3].astype(int), return_counts=True)\n",
    "                    self.experience_dict[s][a][\"count\"][unique] = counts\n",
    "    \n",
    "    def estimate_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        Generate transition matrix of shape S x A x S.\n",
    "\n",
    "        Should be called before create_transition_dict.\n",
    "        \"\"\"\n",
    "        # Experience dictionary was not yet created\n",
    "        if self.experience_dict is None:\n",
    "            self._create_experience_dict()\n",
    "\n",
    "        T = np.zeros(shape=(self.nS, self.nA, self.nT, self.nS), dtype=float)\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                ### Binomial likelihood\n",
    "                binomial_likelihood = self.experience_dict[s][a]['count'] / self.experience_dict[s][a]['count'].sum()\n",
    "                T[s, a, 0, :] = binomial_likelihood\n",
    "        \n",
    "        self.T = T\n",
    "\n",
    "        return T\n",
    "    \n",
    "    def create_transition_dict(self):\n",
    "        \"\"\"\n",
    "        Should be called after estimate_transition_matrix.\n",
    "        \"\"\"\n",
    "        # Check if we have a transition matrix to work with\n",
    "        if self.T is None or sum(sum(sum(sum(self.T)))).sum() < 1:\n",
    "            raise Exception(\"Transistion matrix is not yet created OR all entries are zeros.\")\n",
    "        \n",
    "        # Check for terminal \n",
    "        is_done = lambda s: s in self.terminal_states\n",
    "        \n",
    "#         def _check_reward(model, state, next_state, action):\n",
    "#             if state == 0 and action == 0 and next_state == 1:\n",
    "#                 return model.reward_left_good\n",
    "#             elif state == 0 and action == 0 and next_state == 2:\n",
    "#                 return model.reward_left_bad\n",
    "#             # One option left state == 0 and action == 1 and next_state == 3\n",
    "#             else:\n",
    "#                 return model.reward\n",
    "\n",
    "        def _check_reward(model, state, next_state, action):\n",
    "            if state == 0 and next_state == 1:\n",
    "                return model.reward_left_good\n",
    "            elif state == 0 and next_state == 2:\n",
    "                return model.reward_left_bad\n",
    "            # One option left state == 0 and action == 1 and next_state == 3\n",
    "            else:\n",
    "                return model.reward\n",
    "                \n",
    "        # Fill in transition dictionary\n",
    "        P = {}\n",
    "        for s in range(self.nS):\n",
    "            P[s] = {a: [] for a in range(self.nA)}\n",
    "            \n",
    "            if is_done(s):\n",
    "                P[s][RIGHT] = [(1.0, s, 0, True)]\n",
    "                P[s][LEFT] = [(1.0, s, 0, True)]\n",
    "            else:\n",
    "                for action in [RIGHT, LEFT]:\n",
    "                    P[s][action] = []\n",
    "                    trans_next_states = self.T[s][action][0]\n",
    "                    trans_next_states_indices = np.arange(trans_next_states.shape[0])\n",
    "                    # Check if transition is possible\n",
    "                    next_states = trans_next_states_indices[trans_next_states > 0]\n",
    "                    for ns in next_states:\n",
    "                        P[s][action].append(\n",
    "                            (trans_next_states[ns], ns, _check_reward(self, s, ns, action), is_done(ns)))\n",
    "\n",
    "        # Save the trasition dict too\n",
    "        self.P = P\n",
    "\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romaks/anaconda3/envs/prim/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model = ModelStateGame(experience_tuples=experience)\n",
    "T = model.estimate_transition_matrix()\n",
    "P = model.create_transition_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Agent FP for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDPCustom:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_reward_matrix(env):\n",
    "        \"\"\" \n",
    "        Create the reward matrix of shape S x A x S for the environment model.\n",
    "        \"\"\"\n",
    "        R = np.zeros(shape=(env.nS, env.nA, env.nS), dtype=float)\n",
    "\n",
    "        for s in env.P.keys():\n",
    "            for a in env.P[s].keys():\n",
    "                for trans in env.P[s][a]:\n",
    "                    transition_prob, s_prime, reward, done = trans\n",
    "                    R[s,a,s_prime] = reward\n",
    "        return R\n",
    "    \n",
    "    def __init__(self, env, reward_matrix=None, theta=0.00001, discount_factor=0.99):\n",
    "        self.theta = theta\n",
    "        self.discount_factor = discount_factor\n",
    "        self.iteration = 0\n",
    "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.R = reward_matrix or AgentDPCustom._create_reward_matrix(env)\n",
    "        if env.T is None:\n",
    "            env.generate_transition_matrix()\n",
    "\n",
    "    def train_model_based(self, env, **kwargs):\n",
    "        \"\"\"\n",
    "        Value Iteration Algorithm.\n",
    "\n",
    "        Args:\n",
    "            env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "                env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "                env.nS is a number of states in the environment.\n",
    "                env.nA is a number of actions in the environment.\n",
    "            theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "            discount_factor: Gamma discount factor.\n",
    "\n",
    "        Returns:\n",
    "            A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "        \"\"\"\n",
    "        \n",
    "        for s in range(env.nS):\n",
    "            for a in range(env.nA):\n",
    "                self.Q[s][a] = self.R[s,a,:].dot(env.T[s,a,0,:].T)\n",
    "        \n",
    "        def _to_V(Q):\n",
    "            \"\"\"\n",
    "            Make V values out of Q values.\n",
    "            \"\"\"\n",
    "            V = [np.max(Q[i]) for i in range(0, env.nS)]\n",
    "            # V = np.array(V).reshape(env.shape)\n",
    "\n",
    "            return np.array(V)\n",
    "\n",
    "        def _to_policy_arr(Q):\n",
    "            \"\"\"\n",
    "            Make policy array (as previously in DP) - we have Q values we can take an argmax from them.\n",
    "\n",
    "            Args:\n",
    "                Q (dict): dictionary where keys are state indicies and values are numpy arrays of shape (nA,)\n",
    "            \"\"\"\n",
    "            dict_vals = dict(sorted(Q.items())).values()\n",
    "            policy_arr = np.vstack(list(dict_vals))\n",
    "\n",
    "            return policy_arr\n",
    "\n",
    "        return _to_policy_arr(self.Q), _to_V(self.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Agents and Model Learning Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experience(env, num_experience_tuples):\n",
    "    \"\"\"\n",
    "    Learn a model for different number of experience tuples\n",
    "\n",
    "    Args:\n",
    "        env: gridworld environment\n",
    "\n",
    "    Returns:\n",
    "        list, list of tuples of the form (x,y, action, reward, next_state)\n",
    "    \"\"\"\n",
    "    experience = []\n",
    "    \n",
    "    # Make sure we visited all the states at least once\n",
    "    for s in range(env.nS):\n",
    "        for a in range(env.nA):\n",
    "            _ = env.reset()\n",
    "            env.s = s \n",
    "            action = a\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Record experience\n",
    "            experience.append((s, action, reward, next_state))\n",
    "    \n",
    "    # Generate an episode\n",
    "    for t in range(num_experience_tuples): \n",
    "        state = env.reset()\n",
    "        # Random action \n",
    "        action = np.random.randint(2)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Record experience\n",
    "        experience.append((state, action, reward, next_state))\n",
    "\n",
    "    return experience\n",
    "\n",
    "\n",
    "def learn_model(env, experience, seed=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: true model, used to accurately make reward function (as it is assumed known)\n",
    "    \"\"\"\n",
    "    model = ModelStateGame(experience_tuples=experience, seed=seed)\n",
    "    T = model.estimate_transition_matrix()\n",
    "    P = model.create_transition_dict()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 1\n",
    "experience_volumes = EXPERIENCE_VOLUMES\n",
    "# Create true model\n",
    "env = StateGame()\n",
    "P = env.P\n",
    "T_true = env.generate_transition_matrix()\n",
    "\n",
    "# Store for plotting\n",
    "model_divergence_errors_final = []\n",
    "model_big_errs_count_final = []\n",
    "\n",
    "# Plan for each experiene volume\n",
    "for n_experience_tuples in experience_volumes:\n",
    "    # Make plot to see if there is convergence\n",
    "    model_divergence_errors = []\n",
    "    model_big_errs_count = []\n",
    "    for _ in range(n_experiments):\n",
    "        seed = SEED #int(np.random.rand() * 1000000)\n",
    "        experience = generate_experience(env, n_experience_tuples)\n",
    "        model = learn_model(env, experience, seed=seed)       \n",
    "        # See how well the model was estimated \"total probability mass error\"\n",
    "        divergence_value = sum(sum(abs(T_true - model.T))).sum()\n",
    "        # See which transition probabilities are different by more than 1 p.p.\n",
    "        T_masked = ma.masked_where(model.T > 0, model.T)\n",
    "        T_true_masked = ma.masked_where(T_true > 0, T_true)\n",
    "        count = 0\n",
    "        for state in range(env.nS):\n",
    "            for action in range(env.nA):\n",
    "                if (abs(T_masked[state, action, 0, :].data - T_true_masked[state, action, 0, :].data).sum() < 0.01):\n",
    "                    pass\n",
    "                else:\n",
    "                    count += 1\n",
    "        # Record for evaluation/plotting\n",
    "        model_divergence_errors.append(divergence_value)\n",
    "        model_big_errs_count.append(count)\n",
    "        \n",
    "    model_divergence_errors_final.append(sum(model_divergence_errors)/n_experiments)\n",
    "    model_big_errs_count_final.append(sum(model_big_errs_count)/n_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **High bias for Q-learning** (especcially when choosing inaccurate learning hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Q_s0_a0 =  0.9\n",
      "True Q_s0_a1 =  1\n",
      "Estimated Q_s0_a0 =  0.896519692140362\n",
      "Estimated Q_s0_a1 =  0.9932217164504324\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#####################################################\n",
    "# Choosing training setting for Q-learning\n",
    "#####################################################\n",
    "\n",
    "# Create true model\n",
    "env = StateGame()\n",
    "env.reset()\n",
    "\n",
    "agent = AGENT(env) # AgentQLearning(env) # \n",
    "## When want to test a learn model with epsilon/ci bounds\n",
    "#env = #copy(model_robust)\n",
    "    \n",
    "Q_s0_a0 = []\n",
    "Q_s0_a1 = []\n",
    "n_trials = 100\n",
    "for i in range(n_trials):\n",
    "    # Perfect knowledge agent\n",
    "    policy_perfect, V_perfect = agent.train_model_based(env, num_episodes=2000, discount_factor=1.0, alpha=0.005, epsilon=1.0)\n",
    "    Q_s0_a0.append(agent.Q[0][0])\n",
    "    Q_s0_a1.append(agent.Q[0][1])   \n",
    "\n",
    "_, _ = plt.subplots()\n",
    "plt.hist(Q_s0_a0)\n",
    "_, _ = plt.subplots()\n",
    "plt.hist(Q_s0_a1)\n",
    "\n",
    "print(\"True Q_s0_a0 = \", env.reward_left_good*env.nu_left + (1-env.nu_left) * env.reward_left_bad )\n",
    "print(\"True Q_s0_a1 = \", env.reward * env.nu_right)\n",
    "\n",
    "print(\"Estimated Q_s0_a0 = \", sum(Q_s0_a0) / n_trials)\n",
    "print(\"Estimated Q_s0_a1 = \", sum(Q_s0_a1) / n_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Incorporate $\\epsilon$-robustness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_reward_matrix(env):\n",
    "#     \"\"\" \n",
    "#     Create the reward matrix of shape S x A x S for the environment model.\n",
    "#     \"\"\"\n",
    "#     R = np.zeros(shape=(env.nS, env.nA, env.nS), dtype=float)\n",
    "    \n",
    "#     for s in range(env.nS):  # env.P.keys():\n",
    "#         for a in range(env.nA):  # env.P[s].keys():\n",
    "#             for trans in env.P[s][a]:\n",
    "#                 transition_prob, s_prime, reward, done = trans\n",
    "#                 # If not terminal\n",
    "#                 if s != s_prime and s_prime == 1:\n",
    "#                     R[s,a,s_prime] = env.reward_left_good\n",
    "#                 else:\n",
    "#                     R[s,a,s_prime] = reward\n",
    "#     return R\n",
    "\n",
    "def create_reward_matrix(env):\n",
    "    \"\"\" \n",
    "    Create the reward matrix of shape S x A x S for the environment model.\n",
    "    \"\"\"\n",
    "    R = np.zeros(shape=(env.nS, env.nA, env.nS), dtype=float)\n",
    "    \n",
    "    for s in range(env.nS):  # env.P.keys():\n",
    "        for a in range(env.nA):  # env.P[s].keys():\n",
    "            for s_prime in range(env.nS):\n",
    "                \n",
    "                def _check_reward(model, state, next_state, action):\n",
    "                    if state != 0:\n",
    "                        return 0 \n",
    "                    if state == 0 and next_state == 1:\n",
    "                        return model.reward_left_good\n",
    "                    elif state == 0 and next_state == 2:\n",
    "                        return model.reward_left_bad\n",
    "                    # One option left state == 0 and action == 1 and next_state == 3\n",
    "                    else:\n",
    "                        return model.reward\n",
    "                \n",
    "                R[s,a,s_prime] = _check_reward(env, s, s_prime, a)\n",
    "                \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = create_reward_matrix(env)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Run robust planning for the worst-case transition within L1 square**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nu_s_a_hat_min(state, action, T, R, epsilon_robust):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        R (numpy.array): should be derived from model \n",
    "    \"\"\"\n",
    "    nu_s_a = T[state, action].squeeze().copy()\n",
    "    r_s_a = R[state, action].squeeze().copy()\n",
    "    \n",
    "    calc_expected_gain = lambda nu_s_a_hat: nu_s_a_hat.dot(r_s_a.T)\n",
    "    \n",
    "    def _calc_distance_l1(nu_s_a_hat):\n",
    "        \"\"\"\n",
    "        Works with the optimization task. \n",
    "        Note, to shift 0.5 mass from one bin to another epsilon should be 1: \n",
    "           - because the other bin changes mass and that is accounted.\n",
    "           \n",
    "        TODO: nu_s_a is defined outside of the function\n",
    "        \"\"\"\n",
    "        return - cdist(nu_s_a_hat.reshape((1,-1)), nu_s_a.reshape((1,-1)), 'cityblock')[0] + epsilon_robust\n",
    "    \n",
    "    # Constraints\n",
    "    distance_constraint = {'type': 'ineq', 'fun': _calc_distance_l1}\n",
    "    # We nu_s_a is a pmf, so entries should sum to one \n",
    "    calc_pmf_sum = lambda nu_s_a_hat: nu_s_a_hat.sum() - 1\n",
    "    pmf_constraint = {'type': 'eq', 'fun': calc_pmf_sum}\n",
    "    # All entries nonnegative \n",
    "    bounds = tuple([(0, 1) for _ in nu_s_a])\n",
    "\n",
    "    res = minimize(calc_expected_gain, nu_s_a, method='trust-constr', constraints=[pmf_constraint, distance_constraint], options={'verbose': 0, 'disp':False}, bounds=bounds)\n",
    "    \n",
    "    ## DIRTY DEALING WITH NUMERICAL STABILITY \n",
    "    #     if sum(res.x) != 1:\n",
    "    #         logging.warning(f\"Nu_s_a probabilities do not sum up to 1, but to {sum(res.x)}\")\n",
    "    #         logging.warning(f\" After adjustment: {sum(res.x + (1 - sum(res.x)) / len(res.x))}\")\n",
    "    #         if sum(res.x + (1 - sum(res.x)) / len(res.x)) != 1:\n",
    "    #             logging.warning(f\"Even now they do not sum to 1, but to {sum(res.x)}\")\n",
    "    #             logging.warning( sum(res.x + (1 - sum(res.x)) / len(res.x)) )\n",
    "    #             return res.x + np.finfo(float).eps if (sum(res.x))<1 else res.x-np.finfo(float).eps\n",
    "    #         return res.x + (1 - sum(res.x)) / len(res.x)\n",
    "    \n",
    "    return res.x.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romaks/anaconda3/envs/prim/lib/python3.7/site-packages/scipy/optimize/_hessian_update_strategy.py:187: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  'approximations.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.54068973e-05 4.01226190e-01 5.98722996e-01 2.54068973e-05]\n",
      "[3.42704319e-05 1.05048633e-05 4.99039020e-02 9.50051323e-01]\n"
     ]
    }
   ],
   "source": [
    "# Create the true model\n",
    "env = StateGame()\n",
    "P = env.P\n",
    "T_true = env.generate_transition_matrix()\n",
    "\n",
    "# Try to learn the model \n",
    "n_experience_tuples = 1000\n",
    "experience = generate_experience(env, n_experience_tuples)\n",
    "model = learn_model(env, experience, seed=None)\n",
    "\n",
    "# print(\"--------- Model transition dict ---------\")\n",
    "# pp.pprint(model.experience_dict)\n",
    "# print()\n",
    "# print(\"--------- Estimated T ---------\")\n",
    "# print(model.T)\n",
    "# print()\n",
    "\n",
    "def get_epsilon_robust_model(model, epsilon_robust):\n",
    "    \"\"\" \n",
    "    Returns learnt model where T is adjusted to the worst-case scenario.\n",
    "    \"\"\"\n",
    "    # Create dynamics matrix which will hold the worst transition dynamics (on the distance radius epsilon_robust)\n",
    "    T_min = model.T.copy()\n",
    "\n",
    "    # Get reward matrix\n",
    "    R = create_reward_matrix(model)\n",
    "    \n",
    "    # Given that our distance metric is just L1 (abolute diffs):\n",
    "    #   For each nonterminal state and action, I want to find nu_s_a that minimizes \"expected gain\" (to be defined later) \n",
    "    for state in [0]:\n",
    "        for action in range(env.nA):\n",
    "            T_min[state,action,0,:] = get_nu_s_a_hat_min(state, action, model.T.copy(), R.copy(), epsilon_robust)\n",
    "            \n",
    "            print(get_nu_s_a_hat_min(state, action, model.T.copy(), R.copy(), epsilon_robust))\n",
    "            # Modify transition probabilities in the model to T_min\n",
    "            model.T = T_min.copy()\n",
    "            # Modifies P in `model`\n",
    "            model.create_transition_dict()        \n",
    "    \n",
    "    return model\n",
    "\n",
    "# epsilon for the ball around reference dynamics\n",
    "epsilon_robust = 0.1\n",
    "# Robust model \n",
    "model_robust = get_epsilon_robust_model(copy(model), epsilon_robust)\n",
    "        \n",
    "# print(f\"--------- Worst-case estimated T for epsilon = {epsilon_robust} ---------\")\n",
    "# print(model_robust.T)\n",
    "# print()\n",
    "        \n",
    "# # Record estimated Q value\n",
    "agent = AGENT(model_robust) # AgentQLearning(model_robust) #\n",
    "# Usually you want to run the agent untill convergence\n",
    "policy, V = agent.train_model_based(env=model_robust, num_episodes=10000, alpha=0.01, epsilon=0.5)\n",
    "\n",
    "# Why do we have Q[0][1] > 1 ?\n",
    "# print(agent.Q[0][0])\n",
    "# print(agent.Q[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run imaginary `main()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_DEGRAD_DICT = {201: 0.05, 501: 0.05, 10001: 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of experiments per each experience volume \n",
    "n_experiments = 10\n",
    "seeds = list(map(int, np.absolute((np.random.rand(n_experiments) * 1000)).tolist()))\n",
    "# [10, 20, 50, 70, 100, 200, 500, 1000, 2000, 4000] \n",
    "experience_volumes = EXPERIENCE_VOLUMES\n",
    "# Create true model\n",
    "env = StateGame()\n",
    "P = env.P\n",
    "T_true = env.generate_transition_matrix()\n",
    "\n",
    "# Store for plotting\n",
    "model_divergence_errors_final = np.zeros((1,n_experiments))\n",
    "model_big_errs_count_final = np.zeros((1,n_experiments))\n",
    "Q_vals_s0_a0_final = np.zeros((1,n_experiments)) \n",
    "Q_vals_s0_a1_final = np.zeros((1,n_experiments)) \n",
    "nu_vals_final = np.zeros((1,n_experiments))\n",
    "# Robust means 'worst-case'\n",
    "Q_vals_robust_s0_a0_final = np.zeros((1,n_experiments)) \n",
    "Q_vals_robust_s0_a1_final = np.zeros((1,n_experiments)) \n",
    "nu_vals_robust_final = np.zeros((1,n_experiments))\n",
    "nu_a1_vals_robust_final = np.zeros((1,n_experiments))\n",
    "\n",
    "# Plan for each experiene volume\n",
    "for n_experience_tuples in experience_volumes:\n",
    "    # Make plot to see if there is convergence\n",
    "    model_divergence_errors = []\n",
    "    model_big_errs_count = []\n",
    "    # In real setting we need to evaluate on a path, then collect returns\n",
    "    Q_vals_s0_a0 = [] \n",
    "    Q_vals_s0_a1 = [] \n",
    "    nu_vals = []\n",
    "    Q_vals_robust_s0_a0 = [] \n",
    "    Q_vals_robust_s0_a1 = [] \n",
    "    nu_vals_robust = []\n",
    "    nu_a1_vals_robust = []\n",
    "    \n",
    "    for i in range(n_experiments):\n",
    "        seed = None #seeds[i]\n",
    "        experience = generate_experience(env, n_experience_tuples)\n",
    "        model = learn_model(env, experience, seed=seed)\n",
    "        nu_vals.append(model.P[0][0][0][0])\n",
    "        \n",
    "        # Run a planning algorithm with the available model\n",
    "        agent = AGENT(model) #AgentDPCustom(model) #AgentQLearning(model) # \n",
    "        alpha_lvl = np.digitize(n_experience_tuples, ALPHA_LIST)\n",
    "        alpha = ALPHA_DEGRAD_DICT[list(ALPHA_DEGRAD_DICT.keys())[alpha_lvl-1]]\n",
    "        num_episodes = 500 if alpha >0.01 else 1000  # n_experience_tuples  # \n",
    "        # Usually you want to run the agent untill convergence\n",
    "        policy, V = agent.train_model_based(env=model, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "        # Record estimated Q value\n",
    "        Q_vals_s0_a0.append(agent.Q[0][0])\n",
    "        Q_vals_s0_a1.append(agent.Q[0][1])\n",
    "\n",
    "        # Record for evaluation/plotting\n",
    "        model_divergence_errors.append(divergence_value)\n",
    "        model_big_errs_count.append(count)\n",
    "        \n",
    "        # Run robust planning\n",
    "        epsilon_robust = 0.1\n",
    "        model_robust = get_epsilon_robust_model(model, epsilon_robust)\n",
    "        \n",
    "        ## Because of numerical optimization: discrete probability functions nu_s_a might not sum to 1\n",
    "        #if model_robust.T.sum().sum().sum().sum() != 8:\n",
    "        #logging.warning(f\"Wrong transition matrix. Nu_hats are do not sum to correct val. sum(T) = {model_robust.T.sum().sum().sum().sum().sum()}\")\n",
    "        \n",
    "        # Record nu worst-case\n",
    "        nu_vals_robust.append(model_robust.P[0][0][1][0])\n",
    "        nu_a1_vals_robust.append(model_robust.P[0][1][3][0])\n",
    "        # Run the same planning algorithm with the available model\n",
    "        agent = AGENT(model_robust) #AgentDPCustom(model_robust) # AgentQLearning(model_robust) #\n",
    "        # Usually you want to run the agent untill convergence\n",
    "        policy, V = agent.train_model_based(env=model_robust, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "        \n",
    "        # Record estimated Q value\n",
    "        Q_vals_robust_s0_a0.append(agent.Q[0][0])\n",
    "        Q_vals_robust_s0_a1.append(agent.Q[0][1])\n",
    "          \n",
    "    print(\"N experience tuples: \", n_experience_tuples)\n",
    "    print(\"Estimate of robust s_0: \", model_robust.P[0])\n",
    "    \n",
    "    # What we will be plotting \n",
    "#     model_divergence_errors_final.append(sum(model_divergence_errors)/n_experiments)\n",
    "#     model_big_errs_count_final.append(sum(model_big_errs_count)/n_experiments)\n",
    "#     Q_vals_s0_a0_final.append(sum(Q_vals_s0_a0)/n_experiments) \n",
    "#     Q_vals_s0_a1_final.append(sum(Q_vals_s0_a1)/n_experiments) \n",
    "#     nu_vals_final.append(sum(nu_vals)/n_experiments)\n",
    "    \n",
    "#     Q_vals_robust_s0_a0_final.append(sum(Q_vals_robust_s0_a0)/n_experiments) \n",
    "#     Q_vals_robust_s0_a1_final.append(sum(Q_vals_robust_s0_a1)/n_experiments) \n",
    "#     nu_vals_robust_final.append(sum(nu_vals_robust)/n_experiments)\n",
    "#     nu_a1_vals_robust_final.append(sum(nu_a1_vals_robust)/n_experiments)   \n",
    "\n",
    "    model_divergence_errors_final = np.array(np.vstack([model_divergence_errors_final, model_divergence_errors]))\n",
    "    model_big_errs_count_final = np.array(np.vstack([model_big_errs_count_final, model_big_errs_count]))\n",
    "    \n",
    "    Q_vals_s0_a0_final = np.array(np.vstack([Q_vals_s0_a0_final, Q_vals_s0_a0]))\n",
    "    Q_vals_s0_a1_final = np.array(np.vstack([Q_vals_s0_a1_final, Q_vals_s0_a1]))\n",
    "    nu_vals_final = np.array(np.vstack([nu_vals_final, nu_vals]))\n",
    "    \n",
    "    Q_vals_robust_s0_a0_final = np.array(np.vstack([Q_vals_robust_s0_a0_final, Q_vals_robust_s0_a0])) \n",
    "    Q_vals_robust_s0_a1_final = np.array(np.vstack([Q_vals_robust_s0_a1_final, Q_vals_robust_s0_a1])) \n",
    "    nu_vals_robust_final = np.array(np.vstack([nu_vals_robust_final, nu_vals_robust]))\n",
    "    nu_a1_vals_robust_final = np.array(np.vstack([nu_a1_vals_robust_final, nu_a1_vals_robust]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(26,5)) \n",
    "\n",
    "plt.title(f\"State-Game: Evaluating Performance\", fontsize=20)\n",
    "\n",
    "color_a0 = 'tab:red'\n",
    "color_a0_robust = 'tab:orange'\n",
    "color_a1 = 'tab:green'\n",
    "color_a1_robust = 'lime'\n",
    "\n",
    "ax1.plot(x_range, Q_vals_s0_a0_final[1:].mean(axis=1), color=color_a0, label=\"Q(s=0, a=0)\", linewidth=3)\n",
    "# ax1.plot(x_range, Q_vals_s0_a0_final[1:], color=color_a0)\n",
    "# ax1.plot(x_range, Q_vals_s0_a0_final[1:].mean(axis=1) - Q_vals_s0_a0_final[1:].std(axis=1), color=color_a0)\n",
    "# ax1.plot(x_range, Q_vals_s0_a0_final[1:].mean(axis=1) + Q_vals_s0_a0_final[1:].std(axis=1), color=color_a0)\n",
    "ax1.fill_between(x_range, Q_vals_s0_a0_final[1:].mean(axis=1) - Q_vals_s0_a0_final[1:].std(axis=1), \n",
    "                 Q_vals_s0_a0_final[1:].mean(axis=1) + Q_vals_s0_a0_final[1:].std(axis=1), \n",
    "                 color=color_a0, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1), color=color_a0_robust, label=\"Q(s=0, a=0) robust\", linewidth=3)\n",
    "# ax1.plot(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1) - Q_vals_robust_s0_a0_final[1:].std(axis=1), color=color_a0_robust)\n",
    "# ax1.plot(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1) + Q_vals_robust_s0_a0_final[1:].std(axis=1), color=color_a0_robust)\n",
    "ax1.fill_between(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1) - Q_vals_robust_s0_a0_final[1:].std(axis=1), \n",
    "                 Q_vals_robust_s0_a0_final[1:].mean(axis=1) + Q_vals_robust_s0_a0_final[1:].std(axis=1), \n",
    "                 color=color_a0_robust, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_s0_a1_final[1:].mean(axis=1), color=color_a1, label=\"Q(s=0, a=1)\", linewidth=3)\n",
    "# ax1.plot(x_range, Q_vals_s0_a1_final[1:].mean(axis=1) - Q_vals_s0_a1_final[1:].std(axis=1),color=color_a1, )\n",
    "# ax1.plot(x_range, Q_vals_s0_a1_final[1:].mean(axis=1) + Q_vals_s0_a1_final[1:].std(axis=1),color=color_a1, )\n",
    "ax1.fill_between(x_range, Q_vals_s0_a1_final[1:].mean(axis=1) - Q_vals_s0_a1_final[1:].std(axis=1), \n",
    "                 Q_vals_s0_a1_final[1:].mean(axis=1) + Q_vals_s0_a1_final[1:].std(axis=1), \n",
    "                 color=color_a1, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1), color=color_a1_robust, label=\"Q(s=0, a=1) robust\", linewidth=3)\n",
    "# ax1.plot(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1) - Q_vals_robust_s0_a1_final[1:].std(axis=1), color=color_a1_robust)\n",
    "# ax1.plot(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1) + Q_vals_robust_s0_a1_final[1:].std(axis=1), color=color_a1_robust)\n",
    "# ax1.fill_between(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1) - Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "#                  Q_vals_robust_s0_a1_final[1:].mean(axis=1) + Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "#                  color=color_a1_robust, alpha=0.3)\n",
    "\n",
    "# Horizontal line for expected value of Q_s0_a0 and Q_s0_a1\n",
    "ax1.axhline(y=(env.reward_left_good*env.nu_left + (1-env.nu_left)*env.reward_left_bad),\n",
    "            xmin=0, xmax=3, c=color_a0, linewidth=4, zorder=0, alpha=0.4, label=\"Expected Q(s=0, a=0)\")\n",
    "ax1.axhline(y=(env.reward * env.nu_right), xmin=0, xmax=3, c=color_a1, linewidth=4, zorder=0, \n",
    "            alpha=0.2, label=\"Expected Q(s=0, a=1)\")\n",
    "ax1.yaxis.grid(True)\n",
    "ax1.set_ylabel(\"Reward/Return\")\n",
    "\n",
    "ax1.legend(loc='center right', bbox_to_anchor=(1.27, 0.5), ncol=1, fancybox=True, shadow=True)\n",
    "# Choose how xticks presented  \n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "# plt.xticks(x_range, x_ticks)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "\n",
    "fig, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(26,5)) \n",
    "\n",
    "plt.title(f\"State-Game: Learning Transitions\", fontsize=20)\n",
    "\n",
    "color_nu_s0_a0_s1 = 'tab:red'\n",
    "color_nu_s0_a0_s1_worst_case = 'tab:orange'\n",
    "color_a1 = 'tab:green'\n",
    "\n",
    "# We already handled the x-label with ax1\n",
    "ax2.plot(x_range, nu_vals_final[1:].mean(axis=1), color=color_nu_s0_a0_s1, label='Nu left (estimated)', linewidth=3)\n",
    "ax2.fill_between(x_range, nu_vals_final[1:].mean(axis=1) - nu_vals_final[1:].std(axis=1), \n",
    "                 nu_vals_final[1:].mean(axis=1) + nu_vals_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_robust_final[1:].mean(axis=1), color=color_nu_s0_a0_s1_worst_case, label='Nu left (worst-case)', linewidth=3)\n",
    "ax2.fill_between(x_range, nu_vals_robust_final[1:].mean(axis=1) - nu_vals_robust_final[1:].std(axis=1), \n",
    "                 nu_vals_robust_final[1:].mean(axis=1) + nu_vals_robust_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1_worst_case, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, [env.nu_right]*len(x_range), color=color_a1, label='Nu right (estimated)', linewidth=3)\n",
    "\n",
    "ax2.plot(x_range, nu_a1_vals_robust_final[1:].mean(axis=1), color=color_a1_robust, label='Nu right (worst-case)', linewidth=3)\n",
    "ax2.fill_between(x_range, nu_a1_vals_robust_final[1:].mean(axis=1) - nu_a1_vals_robust_final[1:].std(axis=1), \n",
    "                 nu_a1_vals_robust_final[1:].mean(axis=1) + nu_a1_vals_robust_final[1:].std(axis=1), \n",
    "                 color=color_a1, alpha=0.3)\n",
    "\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "\n",
    "ax2.axhline(y=env.nu_left, xmin=0, xmax=3, c=color_nu_s0_a0_s1, linewidth=4, zorder=0, alpha=0.3, label=\"Nu left (true)\")\n",
    "ax2.axhline(y=env.nu_right, xmin=0, xmax=3, c=color_a1, linewidth=4, zorder=0, alpha=0.3, label=\"Nu right (true)\")\n",
    "\n",
    "\n",
    "# Choose how xticks presented  \n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax2.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1, fancybox=True, shadow=True)\n",
    "ax2.yaxis.grid(True)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - (Q-learning) Why are \"robust\" Q value for s0 a1 are bigger than 1? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustness with Confidence Intrevals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nu_s_a_hat_min_ci(model, state, action, T, R, alpha=0.05, cost_matrix=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    nu_s_a = T[state, action].squeeze().copy()\n",
    "    r_s_a = R[state, action].squeeze().copy()\n",
    "    calc_expected_gain = lambda nu_s_a_hat: nu_s_a_hat.dot(r_s_a.T)\n",
    "    \n",
    "    nu_s_a_hat_lower, nu_s_a_hat_upper  = proportion_confint(model.experience_dict[state][action]['count'], model.experience_dict[state][action]['count'].sum(), \n",
    "                                                             alpha=alpha, method='wilson')\n",
    "    \n",
    "    def _calc_distance_l1_lower(nu_s_a_hat):\n",
    "        \"\"\"\n",
    "        Works with the optimization task. \n",
    "        Note, to shift 0.5 mass from one bin to another epsilon should be 1: \n",
    "           - because the other bin changes mass and that is accounted.\n",
    "           \n",
    "        TODO: nu_s_a is defined outside of the function\n",
    "        \"\"\"\n",
    "        return nu_s_a_hat - nu_s_a_hat_lower\n",
    "    \n",
    "    def _calc_distance_l1_upper(nu_s_a_hat):\n",
    "        \"\"\"\n",
    "        Works with the optimization task. \n",
    "        Note, to shift 0.5 mass from one bin to another epsilon should be 1: \n",
    "           - because the other bin changes mass and that is accounted.\n",
    "           \n",
    "        TODO: nu_s_a is defined outside of the function\n",
    "        \"\"\"\n",
    "        return nu_s_a_hat_upper - nu_s_a_hat\n",
    "    \n",
    "    # Constraints\n",
    "    distance_constraint_lower = {'type': 'ineq', 'fun': _calc_distance_l1_lower}\n",
    "    distance_constraint_upper = {'type': 'ineq', 'fun': _calc_distance_l1_upper}\n",
    "    \n",
    "    # We nu_s_a is a pmf, so entries should sum to one \n",
    "    calc_pmf_sum = lambda nu_s_a_hat: nu_s_a_hat.sum() - 1\n",
    "    pmf_constraint = {'type': 'eq', 'fun': calc_pmf_sum}\n",
    "    # All entries nonnegative \n",
    "    bounds = tuple([(0, 1) for _ in nu_s_a])\n",
    "\n",
    "    res = minimize(calc_expected_gain, nu_s_a, method='trust-constr', constraints=[pmf_constraint, distance_constraint_lower, distance_constraint_upper],\n",
    "                   options={'verbose': 0, 'disp':False}, bounds=bounds)\n",
    "    \n",
    "    ## DIRTY DEALING WITH NUMERICAL STABILITY \n",
    "#     if sum(res.x) != 1:\n",
    "#         logger.info(f\"Nu_s_a probabilities do not sum up to 1, but to {sum(res.x)}\")\n",
    "#         logger.info(f\" After adjustment: {sum(res.x + (1 - sum(res.x)) / len(res.x))}\")\n",
    "#         if sum(res.x + (1 - sum(res.x)) / len(res.x)) != 1:\n",
    "#             logger.info(f\"Even now they do not sum to 1, but to {sum(res.x)}\")\n",
    "#             logger.info( sum(res.x + (1 - sum(res.x)) / len(res.x)) )\n",
    "#             return res.x + np.finfo(float).eps if (sum(res.x))<1 else res.x-np.finfo(float).eps\n",
    "#         return res.x + (1 - sum(res.x)) / len(res.x)\n",
    "    \n",
    "    return res.x.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the true model\n",
    "env = StateGame()\n",
    "P = env.P\n",
    "T_true = env.generate_transition_matrix()\n",
    "\n",
    "# Try to learn the model \n",
    "n_experience_tuples = 6000\n",
    "experience = generate_experience(env, n_experience_tuples)\n",
    "model = learn_model(env, experience, seed=None)\n",
    "\n",
    "#print(\"--------- Model transition dict ---------\")\n",
    "#pp.pprint(model.experience_dict)\n",
    "#print(\"--------- Estimated T ---------\")\n",
    "#print(model.T)\n",
    "#print()\n",
    "\n",
    "def get_robust_model_ci(model, alpha):\n",
    "    \"\"\" \n",
    "    Returns learnt model where T is adjusted to the worst-case scenario in Confidence Interval. \n",
    "    \"\"\"\n",
    "    # Create dynamics matrix which will hold the worst transition dynamics (on the distance radius epsilon_robust)\n",
    "    T_min = model.T.copy()\n",
    "\n",
    "    # Get reward matrix\n",
    "    R = create_reward_matrix(model)\n",
    "    \n",
    "    # Given that our distance metric is just L1 (abolute diffs):\n",
    "    #   For each nonterminal state and action, I want to find nu_s_a that minimizes \"expected gain\" (to be defined later) \n",
    "    for state in [0]:\n",
    "        for action in range(env.nA):\n",
    "            T_min[state,action,0,:] = get_nu_s_a_hat_min_ci(model, state, action, model.T.copy(), R.copy(), alpha)\n",
    "            # Modify transition probabilities in the model to T_min\n",
    "            model.T = T_min.copy()\n",
    "            # Modifies P in `model`\n",
    "            model.create_transition_dict()        \n",
    "    \n",
    "    return model\n",
    "\n",
    "## CI @ 1-alpha\n",
    "alpha = 0.1\n",
    "## Robust model \n",
    "model_robust_ci = get_robust_model_ci(copy(model), alpha)\n",
    "        \n",
    "# print(\"--------- Worst-case (in CI) estimated T  ---------\")\n",
    "# print(model_robust_ci.T)\n",
    "# print()\n",
    "        \n",
    "# # Record estimated Q value\n",
    "\n",
    "agent =  AGENT(model_robust_ci) #AgentQLearning(model_robust_ci) #AgentDP() #\n",
    "# Usually you want to run the agent untill convergence\n",
    "policy, V = agent.train_model_based(env=model_robust_ci, num_episodes=500, alpha=0.1, epsilon=1)\n",
    "\n",
    "# Why do we have Q[0][1] > 1 ?\n",
    "# print(agent.Q[0][0])\n",
    "# print(agent.Q[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of experiments per each experience volume \n",
    "n_experiments = 20\n",
    "# [10, 20, 50, 70, 100, 200, 500, 1000, 2000, 4000]\n",
    "experience_volumes = EXPERIENCE_VOLUMES\n",
    "# Create true model\n",
    "env = StateGame()\n",
    "P = env.P\n",
    "T_true = env.generate_transition_matrix()\n",
    "\n",
    "# Store stats\n",
    "model_divergence_errors_final = np.zeros((1, n_experiments))\n",
    "model_big_errs_count_final = np.zeros((1, n_experiments))\n",
    "Q_vals_s0_a0_final = np.zeros((1, n_experiments))\n",
    "Q_vals_s0_a1_final = np.zeros((1, n_experiments))\n",
    "nu_vals_final = np.zeros((1, n_experiments))\n",
    "Q_vals_robust_s0_a0_final = np.zeros((1, n_experiments))\n",
    "Q_vals_robust_s0_a1_final = np.zeros((1, n_experiments))\n",
    "nu_vals_robust_ci_lower_final = np.zeros((1, n_experiments))\n",
    "nu_vals_robust_ci_upper_final = np.zeros((1, n_experiments))\n",
    "nu_vals_a1_robust_ci_lower_final = np.zeros((1, n_experiments))\n",
    "nu_vals_a1_robust_ci_upper_final = np.zeros((1, n_experiments))\n",
    "\n",
    "# Plan for each experiene volume\n",
    "for n_experience_tuples in experience_volumes:\n",
    "    # Make plot to see if there is convergence\n",
    "    model_divergence_errors = []\n",
    "    model_big_errs_count = []\n",
    "    # In real setting we need to evaluate on a path, then collect returns\n",
    "    Q_vals_s0_a0 = [] \n",
    "    Q_vals_s0_a1 = [] \n",
    "    nu_vals = []\n",
    "    Q_vals_robust_s0_a0 = [] \n",
    "    Q_vals_robust_s0_a1 = [] \n",
    "    nu_vals_robust_ci_lower = []\n",
    "    nu_vals_robust_ci_upper = []\n",
    "    nu_vals_a1_robust_ci_lower = []\n",
    "    nu_vals_a1_robust_ci_upper = []\n",
    "    \n",
    "    for _ in range(n_experiments):\n",
    "        seed = None\n",
    "        experience = generate_experience(env, n_experience_tuples)\n",
    "        model = learn_model(env, experience, seed=seed)\n",
    "        # Record nu \n",
    "        nu_vals.append(model.P[0][0][0][0])\n",
    "        \n",
    "        # Run a planning algorithm with the available model\n",
    "        agent = AGENT(model) #AgentDPCustom(model) # AgentQLearning(model) #\n",
    "        alpha_lvl = np.digitize(n_experience_tuples, ALPHA_LIST)\n",
    "        alpha = ALPHA_DEGRAD_DICT[list(ALPHA_DEGRAD_DICT.keys())[alpha_lvl-1]]\n",
    "        num_episodes = 500 if alpha >0.01 else 1000 # n_experience_tuples  # \n",
    "        # Usually you want to run the agent untill convergence\n",
    "        policy, V = agent.train_model_based(env=model, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "        # Record estimated Q value\n",
    "        Q_vals_s0_a0.append(agent.Q[0][0])\n",
    "        Q_vals_s0_a1.append(agent.Q[0][1])\n",
    "       \n",
    "        # To plot later\n",
    "        nu_s_a_hat_lower, nu_s_a_hat_upper = proportion_confint(model.experience_dict[0][0]['count'], model.experience_dict[0][0]['count'].sum(),  \n",
    "                                                                alpha=ALPHA_CI, method='wilson')\n",
    "        nu_s_a1_hat_lower, nu_s_a1_hat_upper = proportion_confint(model.experience_dict[0][1]['count'], model.experience_dict[0][1]['count'].sum(),  \n",
    "                                                                alpha=ALPHA_CI, method='wilson')\n",
    "        # Record nu_s0_a0_s1 CI \n",
    "        nu_vals_robust_ci_lower.append(nu_s_a_hat_lower[1])\n",
    "        nu_vals_robust_ci_upper.append(nu_s_a_hat_upper[1])\n",
    "        nu_vals_a1_robust_ci_lower.append(nu_s_a1_hat_lower[3])\n",
    "        nu_vals_a1_robust_ci_upper.append(nu_s_a1_hat_upper[3])\n",
    "        \n",
    "        # Run robust planning with CI        \n",
    "        model_robust_ci = get_robust_model_ci(copy(model), alpha=alpha)\n",
    "\n",
    "        #if model_robust_ci.T.sum().sum().sum().sum() != 8:\n",
    "        #   logging.warning(f\"Wrong transition matrix. Nu_hats are do not sum to correct val. sum(T) = {model_robust_ci.T.sum().sum().sum().sum().sum()}\")\n",
    "\n",
    "        # Run the same planning algorithm with the available model\n",
    "        agent = AGENT(model_robust_ci)  # AgentDPCustom(model_robust_ci) # AgentQLearning(model_robust_ci) # AgentDP() #\n",
    "        # Usually you want to run the agent untill convergence\n",
    "        policy, V = agent.train_model_based(env=model_robust_ci, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "        \n",
    "        # Record estimated Q value\n",
    "        Q_vals_robust_s0_a0.append(agent.Q[0][0])\n",
    "        Q_vals_robust_s0_a1.append(agent.Q[0][1])\n",
    "          \n",
    "    print(\"N experience tuples: \", n_experience_tuples)\n",
    "    print(\"Estimate of s_0: \", model_robust_ci.P[0])\n",
    "    \n",
    "    # What we will be plotting \n",
    "#     model_divergence_errors_final = np.vstack([model_divergence_errors_final, model_divergence_errors])\n",
    "#     model_big_errs_count_final = np.vstack([model_big_errs_count_final, model_big_errs_count])\n",
    "    Q_vals_s0_a0_final = np.vstack([Q_vals_s0_a0_final, Q_vals_s0_a0]) \n",
    "    Q_vals_s0_a1_final = np.vstack([Q_vals_s0_a1_final, Q_vals_s0_a1]) \n",
    "    nu_vals_final = np.vstack([nu_vals_final, nu_vals])\n",
    "    Q_vals_robust_s0_a0_final = np.vstack([Q_vals_robust_s0_a0_final, Q_vals_robust_s0_a0]) \n",
    "    Q_vals_robust_s0_a1_final = np.vstack([Q_vals_robust_s0_a1_final, Q_vals_robust_s0_a1]) \n",
    "    nu_vals_robust_ci_lower_final = np.vstack([nu_vals_robust_ci_lower_final, nu_vals_robust_ci_lower])\n",
    "    nu_vals_robust_ci_upper_final = np.vstack([nu_vals_robust_ci_upper_final, nu_vals_robust_ci_upper])\n",
    "    nu_vals_a1_robust_ci_lower_final = np.vstack([nu_vals_a1_robust_ci_lower_final, nu_vals_a1_robust_ci_lower])\n",
    "    nu_vals_a1_robust_ci_upper_final = np.vstack([nu_vals_a1_robust_ci_upper_final, nu_vals_a1_robust_ci_upper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(26,5))\n",
    "\n",
    "plt.title(f\"State-Game: Evaluating Performance\", fontsize=25)\n",
    "\n",
    "color_a0 = 'tab:red'\n",
    "color_a0_robust = 'tab:orange'\n",
    "color_a1 = 'tab:green'\n",
    "color_a1_robust = 'lightgreen'\n",
    "\n",
    "ax1.plot(x_range, Q_vals_s0_a0_final[1:].mean(axis=1), color=color_a0, label=\"Q(s=0, a=0)\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_s0_a0_final[1:].mean(axis=1) - Q_vals_s0_a0_final[1:].std(axis=1), \n",
    "                 Q_vals_s0_a0_final[1:].mean(axis=1) + Q_vals_s0_a0_final[1:].std(axis=1), \n",
    "                 color=color_a0, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1), color=color_a0_robust, label=\"Q(s=0, a=0) robust\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1) - Q_vals_robust_s0_a0_final[1:].std(axis=1), \n",
    "                 Q_vals_robust_s0_a0_final[1:].mean(axis=1) + Q_vals_robust_s0_a0_final[1:].std(axis=1), \n",
    "                 color=color_a0_robust, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_s0_a1_final[1:].mean(axis=1), color=color_a1, label=\"Q(s=0, a=1)\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_s0_a1_final[1:].mean(axis=1) - Q_vals_s0_a1_final[1:].std(axis=1), \n",
    "                 Q_vals_s0_a1_final[1:].mean(axis=1) + Q_vals_s0_a1_final[1:].std(axis=1), \n",
    "                 color=color_a1, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1), color=color_a1_robust, label=\"Q(s=0, a=1) robust\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1) - Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "                 Q_vals_robust_s0_a1_final[1:].mean(axis=1) + Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "                 color=color_a1_robust, alpha=0.3)\n",
    "\n",
    "# Horizontal line for expected value of Q_s0_a0 and Q_s0_a1\n",
    "ax1.axhline(y=(env.reward_left_good*env.nu_left + (1-env.nu_left)*env.reward_left_bad),\n",
    "            xmin=0, xmax=3, c=color_a0, linewidth=4, zorder=0, alpha=0.4, label=\"Expected Q(s=0, a=0)\")\n",
    "ax1.axhline(y=(env.reward * env.nu_right), xmin=0, xmax=3, c=color_a1, linewidth=4, zorder=0, \n",
    "            alpha=0.2, label=\"Expected Q(s=0, a=1)\")\n",
    "\n",
    "# Choose how xticks presented  \n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax1.set_ylabel(\"Reward/Return\")\n",
    "\n",
    "ax1.legend(loc='center right', bbox_to_anchor=(1.27, 0.5), ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "fig, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(26,5))\n",
    "\n",
    "# fig.suptitle(f\"Agent's performance for non-robsut and roust planning with learnt models\", fontsize=20)\n",
    "plt.title(f\"State-Game: Learning Transitions\", fontsize=25)\n",
    "\n",
    "color_nu_s0_a0_s1 = 'tab:red'\n",
    "color_nu_s0_a0_s1_worst_case = 'tab:orange'\n",
    "\n",
    "# We already handled the x-label with ax1\n",
    "ax2.plot(x_range, nu_vals_final[1:].mean(axis=1), color=color_nu_s0_a0_s1, label='Nu left (estimated)', linewidth=3)\n",
    "ax2.fill_between(x_range, nu_vals_final[1:].mean(axis=1) - Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "                 nu_vals_final[1:].mean(axis=1) + nu_vals_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_robust_ci_lower_final[1:].mean(axis=1), color=color_nu_s0_a0_s1_worst_case, label='Nu left (lower bound)')\n",
    "ax2.fill_between(x_range, nu_vals_robust_ci_lower_final[1:].mean(axis=1) - nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 nu_vals_robust_ci_lower_final[1:].mean(axis=1) + nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1_worst_case, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_robust_ci_upper_final[1:].mean(axis=1), color=color_nu_s0_a0_s1_worst_case, label='Nu left (upper bound)')\n",
    "ax2.fill_between(x_range, nu_vals_robust_ci_lower_final[1:].mean(axis=1) - nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 nu_vals_robust_ci_lower_final[1:].mean(axis=1) + nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1_worst_case, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, [env.nu_right]*len(x_range), color=color_a1, label='Nu right (estimated)', linewidth=3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_a1_robust_ci_lower_final[1:].mean(axis=1), color=color_a1_robust, label='Nu right (lower bound)')\n",
    "ax2.fill_between(x_range, nu_vals_a1_robust_ci_lower_final[1:].mean(axis=1) - nu_vals_a1_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 nu_vals_a1_robust_ci_lower_final[1:].mean(axis=1) + nu_vals_a1_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 color=color_a1_robust, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_a1_robust_ci_upper_final[1:].mean(axis=1), color=color_a1_robust, label='Nu right (upper bound)')\n",
    "\n",
    "ax2.axhline(y=env.nu_left, xmin=0, xmax=3, c=color_nu_s0_a0_s1, linewidth=4, zorder=0, alpha=0.3, label=\"Nu left (true)\")\n",
    "\n",
    "ax2.axhline(y=env.nu_right, xmin=0, xmax=3, c=color_a1, linewidth=4, zorder=0, alpha=0.3, label=\"Nu right (true)\")\n",
    "\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "\n",
    "# Choose how xticks presented  \n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax2.legend(loc='center right', bbox_to_anchor=(1.27, 0.5), ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "# tight_layout, otherwise the right y-label is slightly clipped\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we need robust planning/policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in the previous examples that planning with learnt models is subjective to the uncertainty surrounding estimated models. Notice oscullations of $Q(s_0, a_0)$ in the first stages of learning: when low number of examples is encountered, the agent may decide to act suboptimally. \n",
    "\n",
    "Next, we inspect what situations result in optimal and suboptimal performances depending on employing non-robust and robust methodology. We measure optimality in terms of regret which is defined as the difference between the expected returns between following the policy that maximizes rewards (optimal policy $\\pi^*$ and the agent's learnt policy.\n",
    "\n",
    "$$ Regret = \\Delta_{G} = G^* - G^{\\pi} $$\n",
    "\n",
    "\n",
    "We hypothise that regret will depend on two factors: action-gap and amount of stochasticity in the system. The action-gap $g^∗_{Q(s)}$ is defined as the difference in (expected) $Q$-values between the optimal action $a_0$ and the nearest sub-optimal action $a_1$ for that state.\n",
    "\n",
    "$$g^∗_{Q(s)} = |Q^∗_{(s_0, a_0)} − Q^∗_{(s_0, a_1)}| $$\n",
    "\n",
    "$Q_{(s_0, a_0)}$ depends on entropy in the system (stochastic transitions) and the reward function. We see that starting from low entropy and increasing it to a certain point decreases the action gap. This point is identified by the ratio between the reward for the best action and reward for the next best action $\\frac{r_{s,a,s'}}{r_{max}}$. \n",
    "\n",
    "In our problem, action gap has the following relationship with $\\nu_{s_0, a_0}$, given rewards $r_{s_0, a_0, s'_1}$, $r_{s_0, a_0, s'_2}=0$ and $r_{s_0, a_1, s'_3}$. Starting from small $\\nu_{s_0, a_0}$ and increasing it, decreases the peformance gap. When $\\nu_{s_0, a_0}$ approaches $\\frac{r_{s_0,a_0,s'_1}}{r_{s_0,a_1,s'_3}}$, the action gap is almost zero, however entropy in the system might be still high. Consider, $\\nu_{s_0, a_0}$ = $\\frac{r_{s_0,a_0,s'_1}}{r_{s_0,a_1,s'_3}} = \\frac{100}{200}$. The action gap is zero, however there is still stochasticity in the system: 50% probability to transition to higher or lower reward state. If $\\nu_{s_0, a_0}$ continues to rise, the peformance gap becomes larger again. But in that case, being robust can have negative consequences as worst-case $\\nu_{s_0, a_0}$ becomes suboptimal if an agent wants to maximize its expected reward. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we calculate regret \n",
    "def get_delta_gain(nu_left_true, nu_left_hat, reward_high, reward, prob_a_switch):\n",
    "    \"\"\"\n",
    "    Logic to calculate 'regret'. Based on the true nu_left and reward functions, we have different regret for robust policies and non-robust policies. \n",
    "    \n",
    "    Args:\n",
    "        nu (float, [0:1]): nu_left probability \n",
    "    \"\"\"\n",
    "    if nu_left_true > prob_a_switch:\n",
    "        a_star = 0\n",
    "        if nu_left_hat * reward_high >= reward:\n",
    "            # a0 is optimal\n",
    "            G_star = nu_left_true * reward_high\n",
    "            G_pi = nu_left_true * reward_high\n",
    "            a_pi = 0\n",
    "        else:\n",
    "            G_star = nu_left_true * reward_high\n",
    "            G_pi = 1 * reward\n",
    "            a_pi = 1\n",
    "    else:\n",
    "        a_star = 1\n",
    "        if nu_left_hat * reward_high >= reward:\n",
    "            # a1 is optimal\n",
    "            G_star = reward\n",
    "            G_pi = nu_left_true * reward_high\n",
    "            a_pi = 0\n",
    "        elif nu_left_hat * reward_high < reward:\n",
    "            G_star = reward\n",
    "            G_pi = reward\n",
    "            a_pi = 1\n",
    "\n",
    "    return G_star - G_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we calculate regret \n",
    "def get_delta_gain(nu_left_true, nu_left_hat, reward_high, reward, prob_a_switch):\n",
    "    \"\"\"\n",
    "    Logic to calculate 'regret'. Based on the true nu_left and reward functions, we have different regret for robust policies and non-robust policies. \n",
    "    \n",
    "    Args:\n",
    "        nu (float, [0:1]): nu_left probability \n",
    "    \"\"\"\n",
    "    if nu_left_true * reward_high > reward:\n",
    "        # a_0 is optimal\n",
    "        G_star = nu_left_true * reward_high\n",
    "    else:\n",
    "        # a_1 is optimal\n",
    "        G_star = reward\n",
    "\n",
    "    a_pi = np.argmax([nu_left_hat * reward_high, reward])\n",
    "\n",
    "    G_pi = [nu_left_true * reward_high, reward][a_pi] \n",
    "    \n",
    "    \n",
    "    return G_star - G_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1 \n",
    "\n",
    "For $\\nu_{s_0, a_0, s_1}$.high entropy and high differences in rewards, we expect to robust policies perform better than non-robust.\n",
    "\n",
    "We evaluate this by: \n",
    "\n",
    "- We sample true $\\nu_{s_0, a_0, s_1}$.\n",
    "- Then, we sample possible 'learnt' models from around the neighborhood of $\\nu_{s_0, a_0, s_1}$.\n",
    "- Next, we calculate regret for each of the 'learnt' models based on non-robust and (epsilon) robust methods.\n",
    "- Record the regrets \n",
    "\n",
    "We measure the differences in rewards as $\\frac{r_{s,a,s'}}{r_{max}}$ i.e. lower ratio corresponds to higher differences in rewards. If $ 0 < \\nu_{s_0, a_0, s_1} \\leq \\frac{r_{s_0, a_1, s_3}}{r_{s_0, a_0, s_1}}$, the system is stochastic and taking a 'risky' action may result in (significantly) higher reward but also lead to zero reward $r_{s_0, a_0, s_2}=0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_high = 3\n",
    "reward = 2.5\n",
    "\n",
    "list_rewards_tuples = [(3,0.5), (3,1), (3,1.5), (3,2.0), (3,2.5), (3, 2.9), (3, 2.95), (3, 2.999)]\n",
    "\n",
    "# Run test this many times\n",
    "n_runs = 10\n",
    "# Number of learn models for each nu_left \n",
    "n_models = 10 \n",
    "# For robust \n",
    "epsilon_robust = 0.2\n",
    "std_nu_left_hat = 0.2\n",
    "std_nu_right_hat = 0.001\n",
    "\n",
    "delta_gain_list = []\n",
    "delta_gain_robust_list = []\n",
    "\n",
    "# For plotting in a loop\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "color_robust = 'tab:green'\n",
    "color = 'tab:red'\n",
    "\n",
    "for idx, x in enumerate(list_rewards_tuples):\n",
    "    reward_high, reward = x\n",
    "    x_idx_axes = idx % 2\n",
    "    y_idx_axes = idx // 2\n",
    "    \n",
    "    # Different ranges of nu\n",
    "    for nu_left_array in  [np.linspace(0, reward/reward_high, 20)]: # [np.linspace(0.0, 1.0, 50)]:  \n",
    "        # Start with low nu's and go to higher nu's\n",
    "        for _ in range(n_runs):\n",
    "            for nu_left_true in nu_left_array:\n",
    "                # Create model with that nu \n",
    "                env = StateGame(nu_left=nu_left_true, reward_left_good=reward_high, reward=reward)\n",
    "                P = env.P\n",
    "                T = env.generate_transition_matrix()\n",
    "\n",
    "                # Sample several nu_hats and create 'learnt' models with them \n",
    "                for learnt_model_idx in range(n_models): \n",
    "\n",
    "                    ################ \n",
    "                    # Non-robust \n",
    "                    ################\n",
    "\n",
    "                    # std_nu represents how many observations you had and stochasticity \n",
    "                    nu_left_hat = np.clip(np.random.normal(nu_left_true, std_nu_left_hat), a_min=0.0, a_max=1)  \n",
    "                    # For deterministic transition sample around probability 1 \n",
    "                    nu_right_hat = np.clip(np.random.normal(1, std_nu_right_hat), a_min=0.0, a_max=1)    \n",
    "                    # Create 'learnt' model\n",
    "                    model = StateGame(nu_left=nu_left_hat, nu_right=nu_right_hat)\n",
    "\n",
    "                    # For each 'learnt' model : record regret (i.e. delta gain)\n",
    "                    delta_gain = get_delta_gain(nu_left_true, nu_left_hat, env.reward_left_good, env.reward, env.reward/env.reward_left_good)\n",
    "                    delta_gain_list.append(delta_gain)\n",
    "\n",
    "                    ################ \n",
    "                    # Epsilon Robust \n",
    "                    ################\n",
    "\n",
    "                    # All the time take the worst \n",
    "                    nu_left_hat_robust = nu_left_hat - epsilon_robust\n",
    "                    nu_right_hat_robust = nu_right_hat - epsilon_robust \n",
    "                    delta_gain_robust = get_delta_gain(nu_left_true, nu_left_hat_robust, env.reward_left_good, env.reward, env.reward/env.reward_left_good)\n",
    "                    delta_gain_robust_list.append(delta_gain_robust)\n",
    "                    \n",
    "        # Plotting\n",
    "        plt.subplot(int(str(24) + str(idx+1)))\n",
    "        n, bins, patches = plt.hist([delta_gain_list, delta_gain_robust_list], label=['non-robust', 'robust'], density=True, color=[color, color_robust])\n",
    "        # Best fit of data: non-robust\n",
    "        (mu, sigma) = norm.fit(delta_gain_list)\n",
    "        y = norm.pdf(bins, mu, sigma)\n",
    "        plt.plot(bins, y, linewidth=2, c=color)\n",
    "        # Best fit of data: robust\n",
    "        (mu, sigma) = norm.fit(delta_gain_robust_list)\n",
    "        y = norm.pdf(bins, mu, sigma)\n",
    "        plt.plot(bins, y, linewidth=2, c=color_robust)\n",
    "        plt.title(f\"Reward ratio {reward/reward_high:.2f}\", fontsize=15)\n",
    "    \n",
    "# Add axis labels as text \n",
    "text(-0.1, 0.6,'Density', ha='center', va='center', transform=ax.transAxes, rotation=90, fontsize=20)\n",
    "text(0.6, -0.1,'Regret',ha='center', va='center', transform=ax.transAxes, rotation=0, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: why do we see that robust policies perform better than non-robust only for some $\\nu_{s_0, a_0}$'s?**\n",
    "\n",
    "<!-- \n",
    "See histograms of frequencies of high regrets for different values of $\\nu_{s_0, a_0}$ below. -->\n",
    "\n",
    "**Answer: when facing similar expected rewards between optimal and suboptimal action, but having high entropy, the agent occures lower regret with robust policies.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action gap vs entropy delta vs regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from scipy.optimize import fsolve, brute,  minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_arr_left = [entropy([x,1-x], base=2) for x in np.linspace(0, 1, 20)]\n",
    "e_arr_right = [entropy([1,0],base=2) for x in np.linspace(0, 1, 20)]\n",
    "d_e_arr = [(np.log2(nu_left) + 1) * (1/np.log2(2)) - (np.log2(1-nu_left) + 1) / np.log2(2) for nu_left in np.linspace(0, 1, 20)]\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 20), e_arr_left)\n",
    "plt.plot(np.linspace(0, 1, 20), e_arr_right)\n",
    "# Derivative of entrpopy for nu_left\n",
    "# plt.plot(np.linspace(0, 1, 20), d_e_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nu_left(e_delta, nu_big=False):\n",
    "    \"\"\"\n",
    "    Get nu_left given entropy delta. We are missing Newton's quadratic convergence but with.\n",
    "    \n",
    "    e_delta should always be given positive. \n",
    "    \n",
    "    If nu_big is True, then nu selected from [0.5:1] else from [0:0.5].  \n",
    "    \"\"\"\n",
    "    ent_delta = lambda nu_left: abs(entropy([nu_left, 1 - nu_left], base=2) - e_delta)        \n",
    "    \n",
    "    res0 = minimize_scalar(ent_delta, bounds=(0.0, 0.5), method='bounded')\n",
    "    \n",
    "    res1 = minimize_scalar(ent_delta, bounds=(0.5, 1), method='bounded')\n",
    "    \n",
    "    return res0.x, res1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_ratio(a_gap, nu_left, left_optimal):\n",
    "    # I can fix this \n",
    "    r_s0_a1_s3 = 1 \n",
    "    if left_optimal:\n",
    "        r_s0_a0_s1 = (a_gap + r_s0_a1_s3) / nu_left\n",
    "    else:\n",
    "        r_s0_a0_s1 = (r_s0_a1_s3 - a_gap) / nu_left\n",
    "        \n",
    "    return r_s0_a0_s1, r_s0_a1_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TESTING ENTROPY AND ACTION RATIO\n",
    "# tups = [(0.1, 0.6), (0.2, 0.1), (0.3, 0.9), (0.4, 0.3), (0.5, 0.5), (0.25, 0.3), (0.15, 0.5)]\n",
    "\n",
    "# for tup in tups:\n",
    "#     # a_0 always optimal\n",
    "#     nu_s, nu_b = get_nu_left(tup[0],True)\n",
    "#     r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(tup[1], nu_s, True)\n",
    "    \n",
    "#     for nu in [nu_s, nu_b]:\n",
    "#         if abs(entropy([nu, 1-nu]) - tup[0]) > 0.1:\n",
    "#             print(\"entrp: \", tup)\n",
    "#         elif r_s0_a0_s1 * nu - r_s0_a1_s3 > tup[1]:\n",
    "#             print(nu)\n",
    "#             print('act gap: ', tup)\n",
    "#             print(r_s0_a0_s1 * nu - r_s0_a1_s3)\n",
    "#             print(r_s0_a0_s1,r_s0_a1_s3 )\n",
    "#             print()\n",
    "#         else:\n",
    "#             print(nu)\n",
    "#             print(r_s0_a0_s1 * nu - r_s0_a1_s3)\n",
    "#             print(r_s0_a0_s1,r_s0_a1_s3 )\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test this many times?\n",
    "n_runs = 100\n",
    "# Number of learnt models for each nu_left \n",
    "n_models = 10 \n",
    "# For robust \n",
    "epsilon_robust = 0.1\n",
    "std_nu_left_hat = 0.1\n",
    "std_nu_right_hat = 0.001\n",
    "\n",
    "results_grid = np.zeros(shape=(1, 4))\n",
    "\n",
    "delta_gain_list = []\n",
    "delta_gain_robust_list = []\n",
    "\n",
    "step_delta_entropy = 0.1\n",
    "step_delta_action = 0.1\n",
    "# First entropy gap, later action gap\n",
    "g = np.mgrid[-1.0:1.1:0.05, 0:1.:0.1]\n",
    "a_e_mesh = list(zip(*(x.flat for x in g)))\n",
    "\n",
    "for entropy_delta, action_gap in a_e_mesh:\n",
    "        if entropy_delta < 0:\n",
    "            nu_left_true_s, nu_left_true_b = get_nu_left(-entropy_delta, nu_big=False)\n",
    "            r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=False)\n",
    "            reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "        else:\n",
    "            nu_left_true_s, nu_left_true_b = get_nu_left(entropy_delta, nu_big=False)\n",
    "            r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=True)\n",
    "            reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "        \n",
    "        if nu_left_true == 0:\n",
    "            print(\"What happened?\")\n",
    "            break\n",
    "            \n",
    "        for nu_left_true in [nu_left_true_s, nu_left_true_b]:\n",
    "            for _ in range(n_runs):\n",
    "                # Create model with that nu \n",
    "                env = StateGame(nu_left=nu_left_true, reward_left_good=reward_high, reward=reward)\n",
    "                P = env.P\n",
    "                T = env.generate_transition_matrix()\n",
    "\n",
    "                # Sample several nu_hats and create 'learnt' models with them \n",
    "                for learnt_model_idx in range(n_models): \n",
    "\n",
    "                    # std_nu represents how many observations you had and stochasticity \n",
    "                    nu_left_hat = np.clip(np.random.normal(nu_left_true, std_nu_left_hat), a_min=0.0, a_max=1)  \n",
    "                    # For deterministic transition sample around probability 1 \n",
    "                    nu_right_hat = np.clip(np.random.normal(1, std_nu_right_hat), a_min=0.0, a_max=1)  \n",
    "                    \n",
    "                    ################ \n",
    "                    # Non-robust \n",
    "                    ################\n",
    "\n",
    "                    # Create 'learnt' model\n",
    "                    #model = StateGame(nu_left=nu_left_hat, nu_right=nu_right_hat)\n",
    "                    # For each 'learnt' model : record regret (i.e. delta gain)\n",
    "                    delta_gain = get_delta_gain(nu_left_true, nu_left_hat, env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "                    delta_gain_list.append(delta_gain)\n",
    "\n",
    "                    ################ \n",
    "                    # Epsilon Robust \n",
    "                    ################\n",
    "\n",
    "                    # All the time take the worst \n",
    "                    nu_left_hat_robust = nu_left_hat - epsilon_robust\n",
    "                    nu_right_hat_robust = nu_right_hat #- epsilon_robust \n",
    "                    delta_gain_robust = get_delta_gain(nu_left_true, nu_left_hat_robust, env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "                    delta_gain_robust_list.append(delta_gain_robust)\n",
    "\n",
    "        # For single nu_left_true we have n_runs estimations of regret\n",
    "        x = action_gap\n",
    "        y = entropy_delta\n",
    "        z_non_robust = sum(delta_gain_list)/len(delta_gain_list)\n",
    "        z_robust = sum(delta_gain_robust_list)/len(delta_gain_robust_list)\n",
    "\n",
    "        result_row = np.array([x,y,z_non_robust, z_robust])\n",
    "        results_grid = np.vstack([results_grid, result_row])\n",
    "\n",
    "# Plot rewards\n",
    "x = results_grid[1:, 0]                   \n",
    "y = results_grid[1:, 1]                    \n",
    "z_non_robust = results_grid[1:, 2] \n",
    "z_robust = results_grid[1:, 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Plot next to each other\n",
    "X, Y = g\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "heatmap = plt.pcolormesh(X,Y,z_robust.reshape(X.shape))\n",
    "plt.colorbar(heatmap)\n",
    "text(-0.1, 0.5,'Entropy Delta', ha='center', va='center', transform=ax.transAxes, rotation=90, fontsize=20)\n",
    "text(0.5, -0.1,'Action Gap', ha='center', va='center', transform=ax.transAxes, rotation=0, fontsize=20)\n",
    "plt.title(\"Robust\", fontsize=25)\n",
    "plt.show()\n",
    "\n",
    "X, Y = g\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "heatmap = plt.pcolormesh(X,Y,z_non_robust.reshape(X.shape))\n",
    "plt.colorbar(heatmap)\n",
    "text(-0.1, 0.5,'Entropy Delta', ha='center', va='center', transform=ax.transAxes, rotation=90, fontsize=20)\n",
    "text(0.5, -0.1,'Action Gap', ha='center', va='center', transform=ax.transAxes, rotation=0, fontsize=20)\n",
    "plt.title(\"Non-robust\", fontsize=25)\n",
    "plt.show()\n",
    "\n",
    "# DIFFERENCES\n",
    "X, Y = g\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "heatmap = plt.pcolormesh(X,Y,z_robust.reshape(X.shape) - z_non_robust.reshape(X.shape))\n",
    "plt.colorbar(heatmap)\n",
    "text(-0.1, 0.5,'Entropy Delta', ha='center', va='center', transform=ax.transAxes, rotation=90, fontsize=20)\n",
    "text(0.5, -0.1,'Action Gap', ha='center', va='center', transform=ax.transAxes, rotation=0, fontsize=20)\n",
    "plt.title(\"Non-robust - Robust\", fontsize=25)\n",
    "plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "color = 'red'\n",
    "color_robust = 'green'\n",
    "\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "surf1 = ax.plot_trisurf(x, y, z_non_robust, linewidth=0.2, antialiased=True, alpha=0.8, label='non-robust', color=color)\n",
    "surf1._facecolors2d=surf1._facecolors3d\n",
    "surf1._edgecolors2d=surf1._edgecolors3d\n",
    "surf2 = ax.plot_trisurf(x, y, z_robust, linewidth=0.2, antialiased=True, alpha=0.8, label='robust', color=color_robust)\n",
    "surf2._facecolors2d=surf2._facecolors3d\n",
    "surf2._edgecolors2d=surf2._edgecolors3d\n",
    "\n",
    "ax.tick_params(axis='x', which='major', pad=10.0, rotation=20)\n",
    "ax.tick_params(axis='y', which='major', pad=0.0, rotation=-40)\n",
    "ax.tick_params(axis='z', which='major', pad=10.0)\n",
    "\n",
    "ax.view_init(10, -20)\n",
    "\n",
    "plt.title(\"Average Regret\", fontsize=20)\n",
    "ax.set_xlabel(\"Action Gap\", labelpad=30)\n",
    "ax.set_ylabel(\"Entropy Delta\", labelpad=40)\n",
    "ax.set_zlabel(\"Regret\", labelpad=27)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.6, .85), ncol=1, fancybox=True, shadow=True)\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating with robust objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_\\mathcal{V}(\\pi) = \\min_{\\nu \\in \\mathcal{V}} \\mathbb{E}_{{\\tau} \\sim p_{\\pi, \\nu}}[G({\\tau}) ] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test this many times?\n",
    "n_runs = 100\n",
    "# Number of learnt models for each nu_left \n",
    "n_models = 10 \n",
    "# For robust \n",
    "epsilon_robust = 0.1\n",
    "std_nu_left_hat = 0.1\n",
    "std_nu_right_hat = 0.001\n",
    "\n",
    "results_grid = np.zeros(shape=(1, 4))\n",
    "\n",
    "delta_gain_list = []\n",
    "delta_gain_robust_list = []\n",
    "\n",
    "step_delta_entropy = 0.1\n",
    "step_delta_action = 0.1\n",
    "# First entropy gap, later action gap\n",
    "g = np.mgrid[-1.0:1.1:0.05, 0:1.:0.1]\n",
    "a_e_mesh = list(zip(*(x.flat for x in g)))\n",
    "\n",
    "for entropy_delta, action_gap in a_e_mesh:\n",
    "        if entropy_delta < 0:\n",
    "            nu_left_true_s, nu_left_true_b = get_nu_left(-entropy_delta, nu_big=False)\n",
    "            r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=False)\n",
    "            reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "        else:\n",
    "            nu_left_true_s, nu_left_true_b = get_nu_left(entropy_delta, nu_big=False)\n",
    "            r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=True)\n",
    "            reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "        \n",
    "        if nu_left_true == 0:\n",
    "            print(\"What happened?\")\n",
    "            break\n",
    "            \n",
    "        for nu_left_true in [nu_left_true_s, nu_left_true_b]:\n",
    "            for _ in range(n_runs):\n",
    "                # Create model with that nu \n",
    "                env = StateGame(nu_left=nu_left_true, reward_left_good=reward_high, reward=reward)\n",
    "                P = env.P\n",
    "                T = env.generate_transition_matrix()\n",
    "\n",
    "                # Sample several nu_hats and create 'learnt' models with them \n",
    "                for learnt_model_idx in range(n_models): \n",
    "\n",
    "                    # std_nu represents how many observations you had and stochasticity \n",
    "                    nu_left_hat = np.clip(np.random.normal(nu_left_true, std_nu_left_hat), a_min=0.0, a_max=1)  \n",
    "                    # For deterministic transition sample around probability 1 \n",
    "                    nu_right_hat = np.clip(np.random.normal(1, std_nu_right_hat), a_min=0.0, a_max=1)  \n",
    "                    \n",
    "                    ################ \n",
    "                    # Epsilon Robust \n",
    "                    ################\n",
    "\n",
    "                    # All the time take the worst \n",
    "                    nu_left_hat_robust = nu_left_hat - epsilon_robust\n",
    "                    nu_right_hat_robust = nu_right_hat #- epsilon_robust \n",
    "                    delta_gain_robust = get_delta_gain(nu_left_hat_robust, nu_left_hat_robust, env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "                    delta_gain_robust_list.append(delta_gain_robust)\n",
    "                    \n",
    "                    ################ \n",
    "                    # Non-robust \n",
    "                    ################\n",
    "\n",
    "                    # Instead of nu_left_true, put the robust estimator\n",
    "                    delta_gain = get_delta_gain(nu_left_hat_robust, nu_left_hat, env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "                    delta_gain_list.append(delta_gain)\n",
    "\n",
    "\n",
    "\n",
    "        # For single nu_left_true we have n_runs estimations of regret\n",
    "        x = action_gap\n",
    "        y = entropy_delta\n",
    "        z_non_robust = sum(delta_gain_list)/len(delta_gain_list)\n",
    "        z_robust = sum(delta_gain_robust_list)/len(delta_gain_robust_list)\n",
    "\n",
    "        result_row = np.array([x,y,z_non_robust, z_robust])\n",
    "        results_grid = np.vstack([results_grid, result_row])\n",
    "\n",
    "# Plot rewards\n",
    "x = results_grid[1:, 0]                   \n",
    "y = results_grid[1:, 1]                    \n",
    "z_non_robust = results_grid[1:, 2] \n",
    "z_robust = results_grid[1:, 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "color = 'red'\n",
    "color_robust = 'green'\n",
    "\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "surf1 = ax.plot_trisurf(x, y, z_non_robust, linewidth=0.2, antialiased=True, alpha=0.8, label='non-robust', color=color)\n",
    "surf1._facecolors2d=surf1._facecolors3d\n",
    "surf1._edgecolors2d=surf1._edgecolors3d\n",
    "surf2 = ax.plot_trisurf(x, y, z_robust, linewidth=0.2, antialiased=True, alpha=0.8, label='robust', color=color_robust)\n",
    "surf2._facecolors2d=surf2._facecolors3d\n",
    "surf2._edgecolors2d=surf2._edgecolors3d\n",
    "\n",
    "ax.tick_params(axis='x', which='major', pad=10.0, rotation=-20)\n",
    "ax.tick_params(axis='y', which='major', pad=0.0, rotation=-0)\n",
    "ax.tick_params(axis='z', which='major', pad=10.0)\n",
    "\n",
    "ax.view_init(15, 20)\n",
    "\n",
    "plt.title(\"Average Regret\", fontsize=20)\n",
    "ax.set_xlabel(\"Action Gap\", labelpad=30)\n",
    "ax.set_ylabel(\"Entropy Delta\", labelpad=25)\n",
    "ax.set_zlabel(\"Regret\", labelpad=27)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.9, .85), ncol=1, fancybox=True, shadow=True)\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of regret evauate performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nu_left(e_delta, nu_big=False):\n",
    "    \"\"\"\n",
    "    Get nu_left given entropy delta. We are missing Newton's quadratic convergence but with.\n",
    "    \n",
    "    e_delta should always be given positive. \n",
    "    \n",
    "    If nu_big is True, then nu selected from [0.5:1] else from [0:0.5].  \n",
    "    \"\"\"\n",
    "    ent_delta = lambda nu_left: abs(entropy([nu_left, 1 - nu_left], base=2) - e_delta)        \n",
    "    \n",
    "    res0 = minimize_scalar(ent_delta, bounds=(0.0, 0.5), method='bounded')\n",
    "    \n",
    "    res1 = minimize_scalar(ent_delta, bounds=(0.5, 1), method='bounded')\n",
    "    \n",
    "    return res0.x, res1.x\n",
    "\n",
    "def get_reward_ratio(a_gap, nu_left, left_optimal):\n",
    "    # I can fix this \n",
    "    r_s0_a1_s3 = 1 \n",
    "    if left_optimal:\n",
    "        r_s0_a0_s1 = (a_gap + r_s0_a1_s3) / nu_left\n",
    "    else:\n",
    "        r_s0_a0_s1 = (r_s0_a1_s3 - a_gap) / nu_left\n",
    "        \n",
    "    return r_s0_a0_s1, r_s0_a1_s3\n",
    "\n",
    "# How we calculate regret \n",
    "def get_delta_gain(nu_left_true, nu_left_hat, reward_high, reward, prob_a_switch):\n",
    "    \"\"\"\n",
    "    Logic to calculate 'regret'. Based on the true nu_left and reward functions, we have different regret for robust policies and non-robust policies. \n",
    "    \n",
    "    Args:\n",
    "        nu (float, [0:1]): nu_left probability \n",
    "    \"\"\"\n",
    "    if nu_left_true * reward_high > reward:\n",
    "        # a_0 is optimal\n",
    "        G_star = nu_left_true * reward_high\n",
    "    else:\n",
    "        # a_1 is optimal\n",
    "        G_star = reward\n",
    "\n",
    "    a_pi = np.argmax([nu_left_hat * reward_high, reward])\n",
    "\n",
    "    G_pi = [nu_left_true * reward_high, reward][a_pi] \n",
    "    \n",
    "    \n",
    "    return G_star - G_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lber(nu_left_est, nu_left_worst_case, reward_high, reward):\n",
    "    \"\"\" \n",
    "    Calculate lower-bound expected return for non-robust and robust method.\n",
    "    \"\"\"    \n",
    "    \n",
    "    if nu_left_worst_case * reward_high > reward:\n",
    "        # a_0 is optimal according to lber\n",
    "        lber_robust = nu_left_worst_case * reward_high\n",
    "        a_lber_opt = 0 \n",
    "    else:\n",
    "        # a_1 is optimal according to lber \n",
    "        lber_robust = reward \n",
    "        a_lber_opt = 1 \n",
    "    \n",
    "    if nu_left_est * reward_high > reward:\n",
    "        # a_0 is optimal\n",
    "        lber_non_robust = nu_left_worst_case * reward_high\n",
    "        a_non_robust_opt = 0 \n",
    "    else:\n",
    "        # a_1 is optimal\n",
    "        lber_non_robust = reward\n",
    "        a_non_robust_opt = 1\n",
    "\n",
    "    return lber_robust, lber_non_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test this many times?\n",
    "n_runs = 100\n",
    "# Number of learnt models for each nu_left \n",
    "n_models = 10 \n",
    "# For robust \n",
    "epsilon_robust = 0.1\n",
    "std_nu_left_hat = 0.1\n",
    "std_nu_right_hat = 0.001\n",
    "\n",
    "results_grid = np.zeros(shape=(1, 4))\n",
    "\n",
    "delta_gain_list = []\n",
    "delta_gain_robust_list = []\n",
    "\n",
    "step_delta_entropy = 0.1\n",
    "step_delta_action = 0.1\n",
    "# First entropy gap, later action gap\n",
    "g = np.mgrid[-1.0:1.1:0.05, 0:1.:0.1]\n",
    "a_e_mesh = list(zip(*(x.flat for x in g)))\n",
    "\n",
    "for entropy_delta, action_gap in a_e_mesh:\n",
    "        if entropy_delta < 0:\n",
    "            nu_left_true_s, nu_left_true_b = get_nu_left(-entropy_delta, nu_big=False)\n",
    "            r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=False)\n",
    "            reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "        else:\n",
    "            nu_left_true_s, nu_left_true_b = get_nu_left(entropy_delta, nu_big=False)\n",
    "            r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=True)\n",
    "            reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "        \n",
    "        if nu_left_true == 0:\n",
    "            print(\"What happened?\")\n",
    "            break\n",
    "            \n",
    "        for nu_left_true in [nu_left_true_s, nu_left_true_b]:\n",
    "            for _ in range(n_runs):\n",
    "                # Create model with that nu \n",
    "                env = StateGame(nu_left=nu_left_true, reward_left_good=reward_high, reward=reward)\n",
    "                P = env.P\n",
    "                T = env.generate_transition_matrix()\n",
    "\n",
    "                # Sample several nu_hats and create 'learnt' models with them \n",
    "                for learnt_model_idx in range(n_models): \n",
    "\n",
    "                    # std_nu represents how many observations you had and stochasticity \n",
    "                    nu_left_hat = np.clip(np.random.normal(nu_left_true, std_nu_left_hat), a_min=0.0, a_max=1)  \n",
    "                    # For deterministic transition sample around probability 1 \n",
    "                    nu_right_hat = np.clip(np.random.normal(1, std_nu_right_hat), a_min=0.0, a_max=1)  \n",
    "                    # All the time take the worst \n",
    "                    nu_left_hat_robust = nu_left_hat - epsilon_robust\n",
    "                    nu_right_hat_robust = nu_right_hat #- epsilon_robust \n",
    "                    \n",
    "                    lber_robust, lber_non_robust = calc_lber(nu_left_est=nu_left_hat, nu_left_worst_case=nu_left_hat_robust, reward_high=env.reward_left_good, reward=env.reward)\n",
    "                    \n",
    "                    delta_gain_robust_list.append(lber_robust)\n",
    "                    \n",
    "                    delta_gain_list.append(lber_non_robust)\n",
    "                    \n",
    "\n",
    "        # For single nu_left_true we have n_runs estimations of regret\n",
    "        x = action_gap\n",
    "        y = entropy_delta\n",
    "        z_non_robust = sum(delta_gain_list)/len(delta_gain_list)\n",
    "        z_robust = sum(delta_gain_robust_list)/len(delta_gain_robust_list)\n",
    "\n",
    "        result_row = np.array([x,y,z_non_robust, z_robust])\n",
    "        results_grid = np.vstack([results_grid, result_row])\n",
    "\n",
    "# Plot rewards\n",
    "x = results_grid[1:, 0]                   \n",
    "y = results_grid[1:, 1]                    \n",
    "z_non_robust = results_grid[1:, 2] \n",
    "z_robust = results_grid[1:, 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "color = 'red'\n",
    "color_robust = 'green'\n",
    "\n",
    "ax = fig.gca(projection=\"3d\")\n",
    "surf1 = ax.plot_trisurf(x, y, z_non_robust, linewidth=0.2, antialiased=True, alpha=0.8, label='non-robust', color=color)\n",
    "surf1._facecolors2d=surf1._facecolors3d\n",
    "surf1._edgecolors2d=surf1._edgecolors3d\n",
    "surf2 = ax.plot_trisurf(x, y, z_robust, linewidth=0.2, antialiased=True, alpha=0.8, label='robust', color=color_robust)\n",
    "surf2._facecolors2d=surf2._facecolors3d\n",
    "surf2._edgecolors2d=surf2._edgecolors3d\n",
    "\n",
    "ax.tick_params(axis='x', which='major', pad=10.0, rotation=-20)\n",
    "ax.tick_params(axis='y', which='major', pad=10.0, rotation=-40)\n",
    "ax.tick_params(axis='z', which='major', pad=10.0)\n",
    "\n",
    "ax.view_init(15, 10)\n",
    "\n",
    "plt.title(\"Average Value\", fontsize=20)\n",
    "ax.set_xlabel(\"Action Gap\", labelpad=30)\n",
    "ax.set_ylabel(\"Entropy Delta\", labelpad=40)\n",
    "ax.set_zlabel(\"Value\", labelpad=20, rotation=90)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.5, .8), ncol=1, fancybox=True, shadow=True)\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a Task: show performace for different number of epxeiences with Robust Objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_delta_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_LEFT_GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NU_LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = None #10203040\n",
    "\n",
    "EXPERIENCE_VOLUMES = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 4000, 6000] \n",
    "ALPHA_CI = 0.1\n",
    "\n",
    "## For each action gap \n",
    "action_gap = 0.1\n",
    "n_samaples = 2 \n",
    "\n",
    "## Define the task \n",
    "entropy_delta_mean = 0.5\n",
    "std_entropy = 0.4\n",
    "entropy_delta_arr = np.random.normal(entropy_delta_mean, std_entropy, n_samaples)\n",
    "\n",
    "# Number of experiments per each experience volume \n",
    "n_experiments = 20\n",
    "# [10, 20, 50, 70, 100, 200, 500, 1000, 2000, 4000]\n",
    "experience_volumes = EXPERIENCE_VOLUMES\n",
    "\n",
    "## Add 1 becaouse you are creating another array with zeros later \n",
    "delta_gain_list_final_2 = np.zeros((1, 1 + len(EXPERIENCE_VOLUMES), n_experiments))\n",
    "delta_gain_robust_list_final_2 = np.zeros((1, 1 + len(EXPERIENCE_VOLUMES), n_experiments))\n",
    "\n",
    "for entropy_delta in entropy_delta_arr:\n",
    "\n",
    "    ## Set the task \n",
    "    nu_left_true_s, nu_left_true_b = get_nu_left(-entropy_delta, nu_big=False)\n",
    "    # Experiment here with small or big, also explain?\n",
    "    nu_left_true = nu_left_true_b\n",
    "    r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=False)\n",
    "    reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "\n",
    "    NU_LEFT =  nu_left_true \n",
    "    REWARD_LEFT_GOOD = r_s0_a0_s1\n",
    "    REWARD_LEFT_BAD = 0\n",
    "    REWARD = r_s0_a1_s3\n",
    "\n",
    "    # Create true model\n",
    "    env = StateGame(nu_left=NU_LEFT,reward_left_good=REWARD_LEFT_GOOD, seed=SEED)\n",
    "    P = env.P\n",
    "    T_true = env.generate_transition_matrix()\n",
    "\n",
    "    # Store stats\n",
    "    model_divergence_errors_final = np.zeros((1, n_experiments))\n",
    "    model_big_errs_count_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_s0_a0_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_s0_a1_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_robust_s0_a0_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_robust_s0_a1_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_robust_ci_lower_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_robust_ci_upper_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_a1_robust_ci_lower_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_a1_robust_ci_upper_final = np.zeros((1, n_experiments))\n",
    "\n",
    "    delta_gain_list_final = np.zeros((1, n_experiments))\n",
    "    delta_gain_robust_list_final = np.zeros((1, n_experiments))\n",
    "\n",
    "    # Plan for each experiene volume\n",
    "    for n_experience_tuples in experience_volumes:\n",
    "        # Make plot to see if there is convergence\n",
    "        model_divergence_errors = []\n",
    "        model_big_errs_count = []\n",
    "        # In real setting we need to evaluate on a path, then collect returns\n",
    "        Q_vals_s0_a0 = [] \n",
    "        Q_vals_s0_a1 = [] \n",
    "        nu_vals = []\n",
    "        Q_vals_robust_s0_a0 = [] \n",
    "        Q_vals_robust_s0_a1 = [] \n",
    "        nu_vals_robust_ci_lower = []\n",
    "        nu_vals_robust_ci_upper = []\n",
    "        nu_vals_a1_robust_ci_lower = []\n",
    "        nu_vals_a1_robust_ci_upper = []\n",
    "\n",
    "        delta_gain_list = []\n",
    "        delta_gain_robust_list = []\n",
    "\n",
    "        for _ in range(n_experiments):\n",
    "            seed = None\n",
    "            experience = generate_experience(env, n_experience_tuples)\n",
    "            model = learn_model(env, experience, seed=seed)\n",
    "            # Record nu \n",
    "            nu_vals.append(model.P[0][0][0][0])\n",
    "\n",
    "            # Run a planning algorithm with the available model\n",
    "            agent = AGENT(model) #AgentDPCustom(model) # AgentQLearning(model) #\n",
    "            alpha_lvl = np.digitize(n_experience_tuples, ALPHA_LIST)\n",
    "            alpha = ALPHA_DEGRAD_DICT[list(ALPHA_DEGRAD_DICT.keys())[alpha_lvl-1]]\n",
    "            num_episodes = 500 if alpha > 0.01 else 1000 # n_experience_tuples  # \n",
    "            # Usually you want to run the agent untill convergence\n",
    "            policy, V = agent.train_model_based(env=model, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "            # Record estimated Q value\n",
    "            Q_vals_s0_a0.append(agent.Q[0][0])\n",
    "            Q_vals_s0_a1.append(agent.Q[0][1])\n",
    "\n",
    "            # To plot later\n",
    "            nu_s_a_hat_lower, nu_s_a_hat_upper = proportion_confint(model.experience_dict[0][0]['count'], model.experience_dict[0][0]['count'].sum(),  \n",
    "                                                                    alpha=ALPHA_CI, method='wilson')\n",
    "            nu_s_a1_hat_lower, nu_s_a1_hat_upper = proportion_confint(model.experience_dict[0][1]['count'], model.experience_dict[0][1]['count'].sum(),  \n",
    "                                                                    alpha=ALPHA_CI, method='wilson')\n",
    "            # Record nu_s0_a0_s1 CI \n",
    "            nu_vals_robust_ci_lower.append(nu_s_a_hat_lower[1])\n",
    "            nu_vals_robust_ci_upper.append(nu_s_a_hat_upper[1])\n",
    "            nu_vals_a1_robust_ci_lower.append(nu_s_a1_hat_lower[3])\n",
    "            nu_vals_a1_robust_ci_upper.append(nu_s_a1_hat_upper[3])\n",
    "\n",
    "            # Run robust planning with CI        \n",
    "            model_robust_ci = get_robust_model_ci(copy(model), alpha=alpha)\n",
    "            #if model_robust_ci.T.sum().sum().sum().sum() != 8:\n",
    "            #   logging.warning(f\"Wrong transition matrix. Nu_hats are do not sum to correct val. sum(T) = {model_robust_ci.T.sum().sum().sum().sum().sum()}\")\n",
    "            # Run the same planning algorithm with the available model\n",
    "            agent = AGENT(model_robust_ci) \n",
    "            # Usually you want to run the agent untill convergence\n",
    "            policy, V = agent.train_model_based(env=model_robust_ci, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "\n",
    "            ## Non-robust \n",
    "            #delta_gain = get_delta_gain(NU_LEFT, model.P[0][0][0][0], env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "            #delta_gain_list.append(delta_gain)\n",
    "\n",
    "            ## Robust \n",
    "            #delta_gain_robust = get_delta_gain(NU_LEFT, nu_s_a_hat_lower[1], env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "            #delta_gain_robust_list.append(delta_gain_robust)\n",
    "\n",
    "            lber_robust, lber_non_robust = calc_lber(nu_left_est=model.P[0][0][0][0], nu_left_worst_case=nu_s_a_hat_lower[1], reward_high=env.reward_left_good, reward=env.reward)\n",
    "            delta_gain_list.append(lber_non_robust)\n",
    "            delta_gain_robust_list.append(lber_robust)\n",
    "        \n",
    "            # Record estimated Q value\n",
    "            Q_vals_robust_s0_a0.append(agent.Q[0][0])\n",
    "            Q_vals_robust_s0_a1.append(agent.Q[0][1])\n",
    "\n",
    "        print(\"N experience tuples: \", n_experience_tuples)\n",
    "        print(\"Estimate of s_0: \", model_robust_ci.P[0])\n",
    "\n",
    "        ## What we will be plotting \n",
    "        #model_divergence_errors_final = np.vstack([model_divergence_errors_final, model_divergence_errors])\n",
    "        #model_big_errs_count_final = np.vstack([model_big_errs_count_final, model_big_errs_count])\n",
    "        Q_vals_s0_a0_final = np.vstack([Q_vals_s0_a0_final, Q_vals_s0_a0]) \n",
    "        Q_vals_s0_a1_final = np.vstack([Q_vals_s0_a1_final, Q_vals_s0_a1]) \n",
    "        nu_vals_final = np.vstack([nu_vals_final, nu_vals])\n",
    "        Q_vals_robust_s0_a0_final = np.vstack([Q_vals_robust_s0_a0_final, Q_vals_robust_s0_a0]) \n",
    "        Q_vals_robust_s0_a1_final = np.vstack([Q_vals_robust_s0_a1_final, Q_vals_robust_s0_a1]) \n",
    "        nu_vals_robust_ci_lower_final = np.vstack([nu_vals_robust_ci_lower_final, nu_vals_robust_ci_lower])\n",
    "        nu_vals_robust_ci_upper_final = np.vstack([nu_vals_robust_ci_upper_final, nu_vals_robust_ci_upper])\n",
    "        nu_vals_a1_robust_ci_lower_final = np.vstack([nu_vals_a1_robust_ci_lower_final, nu_vals_a1_robust_ci_lower])\n",
    "        nu_vals_a1_robust_ci_upper_final = np.vstack([nu_vals_a1_robust_ci_upper_final, nu_vals_a1_robust_ci_upper])\n",
    "\n",
    "        delta_gain_list_final = np.vstack([delta_gain_list_final, delta_gain_list]) \n",
    "        delta_gain_robust_list_final = np.vstack([delta_gain_robust_list_final, delta_gain_robust_list]) \n",
    "    \n",
    "    # For Each \n",
    "    delta_gain_list_final_2 = np.vstack([delta_gain_list_final_2, np.expand_dims(delta_gain_list_final, axis=0)]) \n",
    "    delta_gain_robust_list_final_2 = np.vstack([delta_gain_robust_list_final_2, np.expand_dims(delta_gain_robust_list_final, axis=0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-robust\n",
    "non_robust_means = delta_gain_list_final_2.mean(axis=2).mean(axis=0)\n",
    "# Robust \n",
    "robust_means = delta_gain_robust_list_final_2.mean(axis=2).mean(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "plt.title(f\"Value for s_0: action gap {action_gap} & entropy delta {entropy_delta_mean}\", fontsize=25)\n",
    "\n",
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "# We start from 1st element coz we added 0's as 0th element \n",
    "plt.plot(x_range, non_robust_means[1:], label=\"non-robust\")\n",
    "plt.plot(x_range, robust_means[1:], label=\"robust\")\n",
    "plt.ylabel(\"Average Value\")\n",
    "plt.xlabel(\"Experience tuples\")\n",
    "\n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax.legend(loc='center right', bbox_to_anchor=(1.21, 0.5), ncol=1, fancybox=True, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Non-robust\n",
    "# non_robust_means_neg_entr = delta_gain_list_final_2.mean(axis=2).mean(axis=0)\n",
    "# # Robust \n",
    "# robust_means_neg_entr = delta_gain_robust_list_final_2.mean(axis=2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a Task: show performace/regret for different number of epxeiences with Normal RL Objective\n",
    "\n",
    " - Fix action and entropy gap\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = None #10203040\n",
    "\n",
    "EXPERIENCE_VOLUMES = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 4000, 6000] \n",
    "ALPHA_CI = 0.1\n",
    "\n",
    "# ## Define the task \n",
    "# entropy_delta = -0.5\n",
    "# action_gap = 0.1\n",
    "# ## Set the task \n",
    "# nu_left_true_s, nu_left_true_b = get_nu_left(-entropy_delta, nu_big=False)\n",
    "# # Experiment here with small or big, also explain?\n",
    "# nu_left_true = nu_left_true_s\n",
    "# r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=False)\n",
    "# reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "\n",
    "# NU_LEFT =  nu_left_true \n",
    "# REWARD_LEFT_GOOD = r_s0_a0_s1\n",
    "# REWARD_LEFT_BAD = 0\n",
    "# REWARD = r_s0_a1_s3\n",
    "\n",
    "# print(NU_LEFT)\n",
    "# print(REWARD_LEFT_GOOD)\n",
    "# REWARD_LEFT_GOOD * NU_LEFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For each action gap \n",
    "action_gap = 0.1\n",
    "n_samaples = 2 \n",
    "\n",
    "## Define the task \n",
    "entropy_delta_mean = -0.5\n",
    "std_entropy = 0.4\n",
    "entropy_delta_arr = np.random.normal(entropy_delta_mean, std_entropy, n_samaples)\n",
    "\n",
    "# Number of experiments per each experience volume \n",
    "n_experiments = 20\n",
    "# [10, 20, 50, 70, 100, 200, 500, 1000, 2000, 4000]\n",
    "experience_volumes = EXPERIENCE_VOLUMES\n",
    "\n",
    "## Add 1 becaouse you are creating another array with zeros later \n",
    "delta_gain_list_final_2 = np.zeros((1, 1 + len(EXPERIENCE_VOLUMES), n_experiments))\n",
    "delta_gain_robust_list_final_2 = np.zeros((1, 1 + len(EXPERIENCE_VOLUMES), n_experiments))\n",
    "\n",
    "for entropy_delta in entropy_delta_arr:\n",
    "\n",
    "    ## Set the task \n",
    "    nu_left_true_s, nu_left_true_b = get_nu_left(-entropy_delta, nu_big=False)\n",
    "    # Experiment here with small or big, also explain?\n",
    "    nu_left_true = nu_left_true_s\n",
    "    r_s0_a0_s1, r_s0_a1_s3 = get_reward_ratio(action_gap, nu_left_true, left_optimal=False)\n",
    "    reward_high, reward = r_s0_a0_s1, r_s0_a1_s3\n",
    "\n",
    "    NU_LEFT =  nu_left_true \n",
    "    REWARD_LEFT_GOOD = r_s0_a0_s1\n",
    "    REWARD_LEFT_BAD = 0\n",
    "    REWARD = r_s0_a1_s3\n",
    "\n",
    "    # Create true model\n",
    "    env = StateGame(nu_left=NU_LEFT,reward_left_good=REWARD_LEFT_GOOD, seed=SEED)\n",
    "    P = env.P\n",
    "    T_true = env.generate_transition_matrix()\n",
    "\n",
    "    # Store stats\n",
    "    model_divergence_errors_final = np.zeros((1, n_experiments))\n",
    "    model_big_errs_count_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_s0_a0_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_s0_a1_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_robust_s0_a0_final = np.zeros((1, n_experiments))\n",
    "    Q_vals_robust_s0_a1_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_robust_ci_lower_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_robust_ci_upper_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_a1_robust_ci_lower_final = np.zeros((1, n_experiments))\n",
    "    nu_vals_a1_robust_ci_upper_final = np.zeros((1, n_experiments))\n",
    "\n",
    "    delta_gain_list_final = np.zeros((1, n_experiments))\n",
    "    delta_gain_robust_list_final = np.zeros((1, n_experiments))\n",
    "\n",
    "    # Plan for each experiene volume\n",
    "    for n_experience_tuples in experience_volumes:\n",
    "        # Make plot to see if there is convergence\n",
    "        model_divergence_errors = []\n",
    "        model_big_errs_count = []\n",
    "        # In real setting we need to evaluate on a path, then collect returns\n",
    "        Q_vals_s0_a0 = [] \n",
    "        Q_vals_s0_a1 = [] \n",
    "        nu_vals = []\n",
    "        Q_vals_robust_s0_a0 = [] \n",
    "        Q_vals_robust_s0_a1 = [] \n",
    "        nu_vals_robust_ci_lower = []\n",
    "        nu_vals_robust_ci_upper = []\n",
    "        nu_vals_a1_robust_ci_lower = []\n",
    "        nu_vals_a1_robust_ci_upper = []\n",
    "\n",
    "        delta_gain_list = []\n",
    "        delta_gain_robust_list = []\n",
    "\n",
    "        for _ in range(n_experiments):\n",
    "            seed = None\n",
    "            experience = generate_experience(env, n_experience_tuples)\n",
    "            model = learn_model(env, experience, seed=seed)\n",
    "            # Record nu \n",
    "            nu_vals.append(model.P[0][0][0][0])\n",
    "\n",
    "            # Run a planning algorithm with the available model\n",
    "            agent = AGENT(model) #AgentDPCustom(model) # AgentQLearning(model) #\n",
    "            alpha_lvl = np.digitize(n_experience_tuples, ALPHA_LIST)\n",
    "            alpha = ALPHA_DEGRAD_DICT[list(ALPHA_DEGRAD_DICT.keys())[alpha_lvl-1]]\n",
    "            num_episodes = 500 if alpha > 0.01 else 1000 # n_experience_tuples  # \n",
    "            # Usually you want to run the agent untill convergence\n",
    "            policy, V = agent.train_model_based(env=model, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "            # Record estimated Q value\n",
    "            Q_vals_s0_a0.append(agent.Q[0][0])\n",
    "            Q_vals_s0_a1.append(agent.Q[0][1])\n",
    "\n",
    "            # To plot later\n",
    "            nu_s_a_hat_lower, nu_s_a_hat_upper = proportion_confint(model.experience_dict[0][0]['count'], model.experience_dict[0][0]['count'].sum(),  \n",
    "                                                                    alpha=ALPHA_CI, method='wilson')\n",
    "            nu_s_a1_hat_lower, nu_s_a1_hat_upper = proportion_confint(model.experience_dict[0][1]['count'], model.experience_dict[0][1]['count'].sum(),  \n",
    "                                                                    alpha=ALPHA_CI, method='wilson')\n",
    "            # Record nu_s0_a0_s1 CI \n",
    "            nu_vals_robust_ci_lower.append(nu_s_a_hat_lower[1])\n",
    "            nu_vals_robust_ci_upper.append(nu_s_a_hat_upper[1])\n",
    "            nu_vals_a1_robust_ci_lower.append(nu_s_a1_hat_lower[3])\n",
    "            nu_vals_a1_robust_ci_upper.append(nu_s_a1_hat_upper[3])\n",
    "\n",
    "            # Run robust planning with CI        \n",
    "            model_robust_ci = get_robust_model_ci(copy(model), alpha=alpha)\n",
    "            #if model_robust_ci.T.sum().sum().sum().sum() != 8:\n",
    "            #   logging.warning(f\"Wrong transition matrix. Nu_hats are do not sum to correct val. sum(T) = {model_robust_ci.T.sum().sum().sum().sum().sum()}\")\n",
    "            # Run the same planning algorithm with the available model\n",
    "            agent = AGENT(model_robust_ci) \n",
    "            # Usually you want to run the agent untill convergence\n",
    "            policy, V = agent.train_model_based(env=model_robust_ci, num_episodes=num_episodes, discount_factor=1.0, alpha=alpha, epsilon=1.0, seed=seed)\n",
    "\n",
    "            # Non-robust \n",
    "            delta_gain = get_delta_gain(NU_LEFT, model.P[0][0][0][0], env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "            delta_gain_list.append(delta_gain)\n",
    "\n",
    "            # Robust \n",
    "            delta_gain_robust = get_delta_gain(NU_LEFT, nu_s_a_hat_lower[1], env.reward_left_good, env.reward, env.reward / env.reward_left_good)\n",
    "            delta_gain_robust_list.append(delta_gain_robust)\n",
    "\n",
    "            # Record estimated Q value\n",
    "            Q_vals_robust_s0_a0.append(agent.Q[0][0])\n",
    "            Q_vals_robust_s0_a1.append(agent.Q[0][1])\n",
    "\n",
    "        print(\"N experience tuples: \", n_experience_tuples)\n",
    "        print(\"Estimate of s_0: \", model_robust_ci.P[0])\n",
    "\n",
    "        ## What we will be plotting \n",
    "        #model_divergence_errors_final = np.vstack([model_divergence_errors_final, model_divergence_errors])\n",
    "        #model_big_errs_count_final = np.vstack([model_big_errs_count_final, model_big_errs_count])\n",
    "        Q_vals_s0_a0_final = np.vstack([Q_vals_s0_a0_final, Q_vals_s0_a0]) \n",
    "        Q_vals_s0_a1_final = np.vstack([Q_vals_s0_a1_final, Q_vals_s0_a1]) \n",
    "        nu_vals_final = np.vstack([nu_vals_final, nu_vals])\n",
    "        Q_vals_robust_s0_a0_final = np.vstack([Q_vals_robust_s0_a0_final, Q_vals_robust_s0_a0]) \n",
    "        Q_vals_robust_s0_a1_final = np.vstack([Q_vals_robust_s0_a1_final, Q_vals_robust_s0_a1]) \n",
    "        nu_vals_robust_ci_lower_final = np.vstack([nu_vals_robust_ci_lower_final, nu_vals_robust_ci_lower])\n",
    "        nu_vals_robust_ci_upper_final = np.vstack([nu_vals_robust_ci_upper_final, nu_vals_robust_ci_upper])\n",
    "        nu_vals_a1_robust_ci_lower_final = np.vstack([nu_vals_a1_robust_ci_lower_final, nu_vals_a1_robust_ci_lower])\n",
    "        nu_vals_a1_robust_ci_upper_final = np.vstack([nu_vals_a1_robust_ci_upper_final, nu_vals_a1_robust_ci_upper])\n",
    "\n",
    "        delta_gain_list_final = np.vstack([delta_gain_list_final, delta_gain_list]) \n",
    "        delta_gain_robust_list_final = np.vstack([delta_gain_robust_list_final, delta_gain_robust_list]) \n",
    "    \n",
    "    # For each \n",
    "    delta_gain_list_final_2 = np.vstack([delta_gain_list_final_2, np.expand_dims(delta_gain_list_final, axis=0)]) \n",
    "    delta_gain_robust_list_final_2 = np.vstack([delta_gain_robust_list_final_2, np.expand_dims(delta_gain_robust_list_final, axis=0)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('delta_gain_list_final_2_entr_mean_min_05_std_04_act_gap_01.npy',delta_gain_list_final_2)\n",
    "np.save('delta_gain_robust_list_final_2_entr_mean_min_05_std_04_act_gap_01.npy', delta_gain_robust_list_final_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-robust\n",
    "non_robust_means = delta_gain_list_final_2.mean(axis=2).mean(axis=0)\n",
    "# Robust \n",
    "robust_means = delta_gain_robust_list_final_2.mean(axis=2).mean(axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "plt.title(f\"Regret for action gap {action_gap} & entropy delta mean {entropy_delta_mean}\", fontsize=25)\n",
    "\n",
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "# We start from 1st element coz we added 0's as 0th element \n",
    "plt.plot(x_range, non_robust_means[1:], label=\"non-robust\")\n",
    "plt.plot(x_range, robust_means[1:], label=\"robust\")\n",
    "plt.ylabel(\"Average regret\")\n",
    "plt.xlabel(\"Experience tuples\")\n",
    "\n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax.legend(loc='center right', bbox_to_anchor=(1.21, 0.5), ncol=1, fancybox=True, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "plt.title(f\"Regret for action gap {action_gap} & entropy delta {entropy_delta}\", fontsize=25)\n",
    "\n",
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "# We start from 1st element coz we added 0's as 0th element \n",
    "plt.plot(x_range, delta_gain_list_final.mean(axis=1)[1:], label=\"non-robust\")\n",
    "plt.plot(x_range, delta_gain_robust_list_final.mean(axis=1)[1:], label=\"robust\")\n",
    "plt.ylabel(\"Average regret\")\n",
    "plt.xlabel(\"Experience tuples\")\n",
    "\n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax.legend(loc='center right', bbox_to_anchor=(1.21, 0.5), ncol=1, fancybox=True, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "\n",
    "plt.title(f\"Regret for action gap {action_gap} & entropy delta {entropy_delta}\", fontsize=25)\n",
    "\n",
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "# We start from 1st element coz we added 0's as 0th element \n",
    "plt.plot(x_range, delta_gain_list_final.mean(axis=1)[1:], label=\"non-robust\")\n",
    "plt.plot(x_range, delta_gain_robust_list_final.mean(axis=1)[1:], label=\"robust\")\n",
    "plt.ylabel(\"Average regret\")\n",
    "plt.xlabel(\"Experience tuples\")\n",
    "\n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax.legend(loc='center right', bbox_to_anchor=(1.21, 0.5), ncol=1, fancybox=True, shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evolution of Q values \n",
    "x_ticks = experience_volumes\n",
    "# Change to range if you do not want log scale xaxis\n",
    "x_range = experience_volumes # range(len(x_ticks))\n",
    "\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(26,5))\n",
    "\n",
    "plt.title(f\"State-Game: Evaluating Performance\", fontsize=25)\n",
    "\n",
    "color_a0 = 'tab:red'\n",
    "color_a0_robust = 'tab:orange'\n",
    "color_a1 = 'tab:green'\n",
    "color_a1_robust = 'lightgreen'\n",
    "\n",
    "ax1.plot(x_range, Q_vals_s0_a0_final[1:].mean(axis=1), color=color_a0, label=\"Q(s=0, a=0)\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_s0_a0_final[1:].mean(axis=1) - Q_vals_s0_a0_final[1:].std(axis=1), \n",
    "                 Q_vals_s0_a0_final[1:].mean(axis=1) + Q_vals_s0_a0_final[1:].std(axis=1), \n",
    "                 color=color_a0, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1), color=color_a0_robust, label=\"Q(s=0, a=0) robust\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_robust_s0_a0_final[1:].mean(axis=1) - Q_vals_robust_s0_a0_final[1:].std(axis=1), \n",
    "                 Q_vals_robust_s0_a0_final[1:].mean(axis=1) + Q_vals_robust_s0_a0_final[1:].std(axis=1), \n",
    "                 color=color_a0_robust, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_s0_a1_final[1:].mean(axis=1), color=color_a1, label=\"Q(s=0, a=1)\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_s0_a1_final[1:].mean(axis=1) - Q_vals_s0_a1_final[1:].std(axis=1), \n",
    "                 Q_vals_s0_a1_final[1:].mean(axis=1) + Q_vals_s0_a1_final[1:].std(axis=1), \n",
    "                 color=color_a1, alpha=0.3)\n",
    "\n",
    "ax1.plot(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1), color=color_a1_robust, label=\"Q(s=0, a=1) robust\", linewidth=3)\n",
    "ax1.fill_between(x_range, Q_vals_robust_s0_a1_final[1:].mean(axis=1) - Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "                 Q_vals_robust_s0_a1_final[1:].mean(axis=1) + Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "                 color=color_a1_robust, alpha=0.3)\n",
    "\n",
    "# Horizontal line for expected value of Q_s0_a0 and Q_s0_a1\n",
    "ax1.axhline(y=(env.reward_left_good*env.nu_left),\n",
    "            xmin=0, xmax=3, c=color_a0, linewidth=4, zorder=0, alpha=0.4, label=\"Expected Q(s=0, a=0)\")\n",
    "ax1.axhline(y=(env.reward * env.nu_right), xmin=0, xmax=3, c=color_a1, linewidth=4, zorder=0, \n",
    "            alpha=0.2, label=\"Expected Q(s=0, a=1)\")\n",
    "\n",
    "# Choose how xticks presented  \n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax1.set_ylabel(\"Reward/Return\")\n",
    "\n",
    "ax1.legend(loc='center right', bbox_to_anchor=(1.27, 0.5), ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n",
    "\n",
    "fig, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(26,5))\n",
    "\n",
    "# fig.suptitle(f\"Agent's performance for non-robsut and roust planning with learnt models\", fontsize=20)\n",
    "plt.title(f\"State-Game: Learning Transitions\", fontsize=25)\n",
    "\n",
    "color_nu_s0_a0_s1 = 'tab:red'\n",
    "color_nu_s0_a0_s1_worst_case = 'tab:orange'\n",
    "\n",
    "# We already handled the x-label with ax1\n",
    "ax2.plot(x_range, nu_vals_final[1:].mean(axis=1), color=color_nu_s0_a0_s1, label='Nu left (estimated)', linewidth=3)\n",
    "ax2.fill_between(x_range, nu_vals_final[1:].mean(axis=1) - Q_vals_robust_s0_a1_final[1:].std(axis=1), \n",
    "                 nu_vals_final[1:].mean(axis=1) + nu_vals_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_robust_ci_lower_final[1:].mean(axis=1), color=color_nu_s0_a0_s1_worst_case, label='Nu left (lower bound)')\n",
    "ax2.fill_between(x_range, nu_vals_robust_ci_lower_final[1:].mean(axis=1) - nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 nu_vals_robust_ci_lower_final[1:].mean(axis=1) + nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1_worst_case, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_robust_ci_upper_final[1:].mean(axis=1), color=color_nu_s0_a0_s1_worst_case, label='Nu left (upper bound)')\n",
    "ax2.fill_between(x_range, nu_vals_robust_ci_lower_final[1:].mean(axis=1) - nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 nu_vals_robust_ci_lower_final[1:].mean(axis=1) + nu_vals_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 color=color_nu_s0_a0_s1_worst_case, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, [env.nu_right]*len(x_range), color=color_a1, label='Nu right (estimated)', linewidth=3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_a1_robust_ci_lower_final[1:].mean(axis=1), color=color_a1_robust, label='Nu right (lower bound)')\n",
    "ax2.fill_between(x_range, nu_vals_a1_robust_ci_lower_final[1:].mean(axis=1) - nu_vals_a1_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 nu_vals_a1_robust_ci_lower_final[1:].mean(axis=1) + nu_vals_a1_robust_ci_lower_final[1:].std(axis=1), \n",
    "                 color=color_a1_robust, alpha=0.3)\n",
    "\n",
    "ax2.plot(x_range, nu_vals_a1_robust_ci_upper_final[1:].mean(axis=1), color=color_a1_robust, label='Nu right (upper bound)')\n",
    "\n",
    "ax2.axhline(y=env.nu_left, xmin=0, xmax=3, c=color_nu_s0_a0_s1, linewidth=4, zorder=0, alpha=0.3, label=\"Nu left (true)\")\n",
    "\n",
    "ax2.axhline(y=env.nu_right, xmin=0, xmax=3, c=color_a1, linewidth=4, zorder=0, alpha=0.3, label=\"Nu right (true)\")\n",
    "\n",
    "ax2.set_ylabel(\"Probability\")\n",
    "\n",
    "# Choose how xticks presented  \n",
    "# plt.xticks(x_range, list(map(str,x_ticks)))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Log experience tuples')\n",
    "\n",
    "ax2.legend(loc='center right', bbox_to_anchor=(1.27, 0.5), ncol=1, fancybox=True, shadow=True)\n",
    "\n",
    "# tight_layout, otherwise the right y-label is slightly clipped\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRIM",
   "language": "python",
   "name": "prim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}